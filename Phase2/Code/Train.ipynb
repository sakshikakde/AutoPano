{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/gokul/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/gokul/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/gokul/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/gokul/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/gokul/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/gokul/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from skimage import data, exposure, img_as_float\n",
    "import matplotlib.pyplot as plt\n",
    "from Misc.MiscUtils import *\n",
    "from Misc.DataUtils import *\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import shutil\n",
    "from io import StringIO\n",
    "import string\n",
    "from termcolor import colored, cprint\n",
    "import math as m\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Don't generate pyc codes\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loadData(folder_name, files_in_dir, labels_in_dir, num_training=5000):\n",
    "\n",
    "#     images_data = []\n",
    "#     labels_data = []\n",
    "\n",
    "#     if(len(files_in_dir) < num_training):\n",
    "#         print(\"The data has only \", len(files_in_dir) , \" images and you are trying to get \",num_training, \" images\")\n",
    "#         num_training = len(files_in_dir)\n",
    "\n",
    "#     for n in range(num_training):\n",
    "#         image1_name = folder_name + os.sep + \"Train_synthetic/PA/\" + files_in_dir[n,0]\n",
    "#         image1 = cv2.imread(image1_name)\n",
    "\n",
    "#         image2_name = folder_name + os.sep + \"Train_synthetic/PB/\" + files_in_dir[n,0] \n",
    "#         image2 = cv2.imread(image2_name)\n",
    "\n",
    "#         if(image1 is None) or (image1 is None):\n",
    "#             print(image1_name, \" is empty. Ignoring ...\")\n",
    "#             continue\n",
    "\n",
    "#         image1 = np.float32(image1)\n",
    "#         image2 = np.float32(image2)        \n",
    "\n",
    "\n",
    "#         #combile images along depth\n",
    "\n",
    "#         image = np.dstack((image1, image1))\n",
    "#         # #standardize image from \n",
    "#         # mean = np.mean(image, axis=(1,2), keepdims=True)\n",
    "#         # std = np.std(image, axis=(1,2), keepdims=True)\n",
    "#         # standardized_image = (image - mean) / (std + 0.000001)\n",
    "         \n",
    "\n",
    "#         labels_data.append(labels_in_dir[n,:])\n",
    "\n",
    "#         images_data.append(image)\n",
    "#         # labels_data.append(label)\n",
    "\n",
    "#     return np.array(images_data), np.array(labels_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createDatasetWithBatches(images_data, labels_data, batch_size, shuffle = True):\n",
    "    \n",
    "#     N, B = images_data.shape[0], batch_size\n",
    "#     idx = np.arange(N)\n",
    "#     if shuffle:\n",
    "#         np.random.shuffle(idx)\n",
    "#     return iter((images_data[idx[i:i+B]], labels_data[idx[i:i+B]]) for i in range(0, N, B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataBatches(base_path, files_in_dir, labels_in_dir, batch_size):\n",
    "\n",
    "    images_data = []\n",
    "    labels_data = []\n",
    "\n",
    "    for n in range(batch_size):\n",
    "        random_index = random.randint(0, len(files_in_dir)-1)\n",
    "\n",
    "        image1_name = base_path + os.sep + \"Train_synthetic/PA/\" + files_in_dir[random_index, 0]\n",
    "        image1 = cv2.imread(image1_name)\n",
    "        image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        image2_name = base_path + os.sep + \"Train_synthetic/PB/\" + files_in_dir[random_index, 0] \n",
    "        image2 = cv2.imread(image2_name)\n",
    "        image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        if(image1 is None) or (image1 is None):\n",
    "            print(image1_name, \" is empty. Ignoring ...\")\n",
    "            continue\n",
    "        image1 = np.float32(image1)\n",
    "        image2 = np.float32(image2) \n",
    "        \n",
    "        image1 = cv2.normalize(image1, None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
    "        image2 = cv2.normalize(image2, None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
    "\n",
    "        image = np.dstack((image1, image1))\n",
    "        images_data.append(image)\n",
    "\n",
    "        labels_data.append(labels_in_dir[n,:])\n",
    "\n",
    "    images_data = np.array(images_data)\n",
    "    labels_data = np.array(labels_data)\n",
    "\n",
    "    return images_data, labels_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/gokul/CMSC733/hgokul_p1/Phase2/Data\"\n",
    "CheckPointPath = \"/home/gokul/CMSC733/hgokul_p1/Phase2/Checkpoints/supervised/\"\n",
    "files_in_dir, SaveCheckPoint, ImageSize, NumTrainSamples, labels_in_dir = SetupAll(base_path, CheckPointPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image_dataset, labels = loadData(BasePath, files_in_dir, labels_in_dir, 100)\n",
    "# print(image_dataset.shape)\n",
    "# print(image_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train = createDatasetWithBatches(image_dataset, labels, batch_size = 5, shuffle = True)\n",
    "# dataset_val = dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import  Activation, Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization, InputLayer\n",
    "\n",
    "def ModelInit():\n",
    "\n",
    "    hidden_layer_size, num_classes = 1000, 8\n",
    "    input_shape = (128, 128, 2)\n",
    "    kernel_size = 3\n",
    "    pool_size = 2\n",
    "    filters = 64\n",
    "    dropout = 0.5\n",
    "\n",
    "    initializer = tf.keras.initializers.VarianceScaling(scale=2.0)\n",
    "    layers = [\n",
    "        InputLayer(input_shape),\n",
    "        #128\n",
    "        Conv2D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "#         Conv2D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         MaxPooling2D(pool_size),\n",
    "        \n",
    "#         #64\n",
    "#         Conv2D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         Conv2D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         MaxPooling2D(pool_size),\n",
    "        \n",
    "#         #32\n",
    "#         Conv2D(filters=filters*2, kernel_size=kernel_size, activation='relu', padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         Conv2D(filters=filters*2, kernel_size=kernel_size, activation='relu', padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         MaxPooling2D(pool_size),\n",
    "        \n",
    "#         #16\n",
    "#         Conv2D(filters=filters*2, kernel_size=kernel_size, activation='relu', padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         Conv2D(filters=filters*2, kernel_size=kernel_size, activation='relu', padding='same'),\n",
    "#         BatchNormalization(),\n",
    "#         MaxPooling2D(pool_size),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dropout(dropout),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dropout(dropout),\n",
    "        #for regression model\n",
    "        Dense(8)\n",
    "\n",
    "    ]\n",
    "    model = Sequential(layers)\n",
    "    ######################################################\n",
    "    \n",
    "#     model = Sequential()\n",
    "#     model.add(InputLayer(input_shape))\n",
    "#     ## conv2d 128\n",
    "#     model.add(Conv2D(filters=filters,kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     ## conv2d 128\n",
    "#     model.add(Conv2D(filters=filters,kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     model.add(MaxPooling2D(pool_size))\n",
    "    \n",
    "#     ## conv2d 64\n",
    "#     model.add(Conv2D(filters=filters,kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     ## conv2d 64\n",
    "#     model.add(Conv2D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     model.add(MaxPooling2D(pool_size))\n",
    "    \n",
    "#     ## conv2d 32 2x Filters\n",
    "#     model.add(Conv2D(filters=filters*2,kernel_size=kernel_size, activation='relu', padding='same',))\n",
    "#     model.add(BatchNormalization())\n",
    "#     ## conv2d 32 2x Filters\n",
    "#     model.add(Conv2D(filters=filters*2,kernel_size=kernel_size, activation='relu', padding='same',))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     model.add(MaxPooling2D(pool_size))\n",
    "    \n",
    "#     ## conv2d 16 2x Filters\n",
    "#     model.add(Conv2D(filters=filters*2,\\\n",
    "#             kernel_size=kernel_size, activation='relu', padding='same',))\n",
    "#     model.add(BatchNormalization())\n",
    "#     ## conv2d 16 2x Filters\n",
    "#     model.add(Conv2D(filters=filters*2,\\\n",
    "#             kernel_size=kernel_size, activation='relu', padding='same',))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dropout(dropout))\n",
    "#     model.add(Dense(1024, activation='relu'))\n",
    "#     model.add(Dropout(dropout))\n",
    "#     #for regression model\n",
    "#     model.add(Dense(8))\n",
    "    return model\n",
    "\n",
    "def OptimizerInit():\n",
    "    learning_rate = 0.001\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel(base_path, files_in_dir, labels_in_dir, batch_size, number_of_training_samples, device, model_init_fn, optimizer_init_fn, num_epochs, CheckPointPath, is_training=False): \n",
    "\n",
    "    with tf.device(device):\n",
    "\n",
    "        loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        model = ModelInit()\n",
    "        optimizer = OptimizerInit()\n",
    "        \n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.MeanSquaredError(name='train_accuracy')\n",
    "    \n",
    "        val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "        val_accuracy = tf.keras.metrics.MeanSquaredError(name='val_accuracy')\n",
    "        \n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics\n",
    "            train_loss.reset_states()\n",
    "            train_accuracy.reset_states()\n",
    "\n",
    "            num_terations_per_epoch = int(number_of_training_samples / batch_size)\n",
    "\n",
    "            tf.keras.callbacks.ModelCheckpoint(CheckPointPath, monitor='val_loss', verbose=0, save_best_only=False,\n",
    "            save_weights_only=False, mode='auto')\n",
    "\n",
    "            for iteration in tqdm(range(num_terations_per_epoch)):\n",
    "            \n",
    "                images_batch, labels_batch = loadDataBatches(base_path, files_in_dir, labels_in_dir, batch_size)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    \n",
    "                    # Use the model function to build the forward pass.\n",
    "                    scores = model(images_batch, training=is_training)\n",
    "                    loss = loss_fn(labels_batch, scores)\n",
    "      \n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                    \n",
    "                    # Update the metrics\n",
    "                    train_loss.update_state(loss)\n",
    "                    train_accuracy.update_state(labels_batch, scores)                    \n",
    "                    print_every = 10\n",
    "                    if t % print_every == 0:\n",
    "                        val_loss.reset_states()\n",
    "                        val_accuracy.reset_states()\n",
    "\n",
    "                        # for test_x, test_y in val_dset:\n",
    "                        #     # During validation at end of epoch, training set to False\n",
    "                        #     prediction = model(test_x, training=False)\n",
    "                        #     t_loss = loss_fn(test_y, prediction)\n",
    "\n",
    "                        #     val_loss.update_state(t_loss)\n",
    "                        #     val_accuracy.update_state(test_y, prediction)\n",
    "                        \n",
    "                        template = 'Iteration {}, Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}'\n",
    "                        print (template.format(t, epoch+1,\n",
    "                                             train_loss.result().numpy(),\n",
    "                                             train_accuracy.result().numpy(),\n",
    "                                             0,\n",
    "                                             0))\n",
    "                    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "number_of_training_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gokul/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:58: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gokul/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:442: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "tf.placeholder() is not compatible with eager execution.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-22974aa51e5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles_in_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_in_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_training_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelInit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptimizerInit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCheckPointPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCheckPointPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-5dd60c117d04>\u001b[0m in \u001b[0;36mTrainModel\u001b[0;34m(base_path, files_in_dir, labels_in_dir, batch_size, number_of_training_samples, device, model_init_fn, optimizer_init_fn, num_epochs, CheckPointPath, is_training)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptimizerInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-31ba0b24d919>\u001b[0m in \u001b[0;36mModelInit\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVarianceScaling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     layers = [\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mInputLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m#128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[1;32m   1345\u001b[0m                                          \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                                          \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m                                          name=self.name)\n\u001b[0m\u001b[1;32m   1348\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(shape, ndim, dtype, sparse, name)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   2138\u001b[0m   \"\"\"\n\u001b[1;32m   2139\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2140\u001b[0;31m     raise RuntimeError(\"tf.placeholder() is not compatible with \"\n\u001b[0m\u001b[1;32m   2141\u001b[0m                        \"eager execution.\")\n\u001b[1;32m   2142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tf.placeholder() is not compatible with eager execution."
     ]
    }
   ],
   "source": [
    "TrainModel(base_path, files_in_dir, labels_in_dir, batch_size, number_of_training_samples, device, ModelInit, OptimizerInit, num_epochs=5, CheckPointPath = CheckPointPath, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
