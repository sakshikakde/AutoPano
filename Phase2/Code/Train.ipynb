{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from skimage import data, exposure, img_as_float\n",
    "import matplotlib.pyplot as plt\n",
    "from Misc.MiscUtils import *\n",
    "from Misc.DataUtils import *\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import shutil\n",
    "from io import StringIO\n",
    "import string\n",
    "from termcolor import colored, cprint\n",
    "import math as m\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Don't generate pyc codes\n",
    "sys.dont_write_bytecode = True\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loadData(folder_name, files_in_dir, labels_in_dir, num_training=5000):\n",
    "\n",
    "#     images_data = []\n",
    "#     labels_data = []\n",
    "\n",
    "#     if(len(files_in_dir) < num_training):\n",
    "#         print(\"The data has only \", len(files_in_dir) , \" images and you are trying to get \",num_training, \" images\")\n",
    "#         num_training = len(files_in_dir)\n",
    "\n",
    "#     for n in range(num_training):\n",
    "#         image1_name = folder_name + os.sep + \"Train_synthetic/PA/\" + files_in_dir[n,0]\n",
    "#         image1 = cv2.imread(image1_name)\n",
    "\n",
    "#         image2_name = folder_name + os.sep + \"Train_synthetic/PB/\" + files_in_dir[n,0] \n",
    "#         image2 = cv2.imread(image2_name)\n",
    "\n",
    "#         if(image1 is None) or (image1 is None):\n",
    "#             print(image1_name, \" is empty. Ignoring ...\")\n",
    "#             continue\n",
    "\n",
    "#         image1 = np.float32(image1)\n",
    "#         image2 = np.float32(image2)        \n",
    "\n",
    "\n",
    "#         #combile images along depth\n",
    "\n",
    "#         image = np.dstack((image1, image1))\n",
    "#         # #standardize image from \n",
    "#         # mean = np.mean(image, axis=(1,2), keepdims=True)\n",
    "#         # std = np.std(image, axis=(1,2), keepdims=True)\n",
    "#         # standardized_image = (image - mean) / (std + 0.000001)\n",
    "         \n",
    "\n",
    "#         labels_data.append(labels_in_dir[n,:])\n",
    "\n",
    "#         images_data.append(image)\n",
    "#         # labels_data.append(label)\n",
    "\n",
    "#     return np.array(images_data), np.array(labels_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createDatasetWithBatches(images_data, labels_data, batch_size, shuffle = True):\n",
    "    \n",
    "#     N, B = images_data.shape[0], batch_size\n",
    "#     idx = np.arange(N)\n",
    "#     if shuffle:\n",
    "#         np.random.shuffle(idx)\n",
    "#     return iter((images_data[idx[i:i+B]], labels_data[idx[i:i+B]]) for i in range(0, N, B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataBatches(base_path, files_in_dir, labels_in_dir, batch_size):\n",
    "\n",
    "    images_data = []\n",
    "    labels_data = []\n",
    "\n",
    "    for n in range(batch_size):\n",
    "        random_index = random.randint(0, len(files_in_dir)-1)\n",
    "\n",
    "        image1_name = base_path + os.sep + \"Train_synthetic/PA/\" + files_in_dir[random_index, 0]\n",
    "        image1 = cv2.imread(image1_name)\n",
    "\n",
    "        image2_name = base_path + os.sep + \"Train_synthetic/PB/\" + files_in_dir[random_index, 0] \n",
    "        image2 = cv2.imread(image2_name)\n",
    "\n",
    "        if(image1 is None) or (image1 is None):\n",
    "            print(image1_name, \" is empty. Ignoring ...\")\n",
    "            continue\n",
    "\n",
    "        image1 = np.float32(image1)\n",
    "        image2 = np.float32(image2) \n",
    "\n",
    "        image = np.dstack((image1, image1))\n",
    "        images_data.append(image)\n",
    "\n",
    "        labels_data.append(labels_in_dir[n,:])\n",
    "\n",
    "    images_data = np.array(images_data)\n",
    "    labels_data = np.array(labels_data)\n",
    "\n",
    "    return images_data, labels_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/sakshi/courses/CMSC733/sakshi_p1/Phase2/Data\"\n",
    "CheckPointPath = \"../Checkpoints/supervised/\"\n",
    "files_in_dir, SaveCheckPoint, ImageSize, NumTrainSamples, labels_in_dir = SetupAll(base_path, CheckPointPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device:  /cpu:0\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "# Constant to control how often we print when training models\n",
    "print_every = 100\n",
    "\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image_dataset, labels = loadData(BasePath, files_in_dir, labels_in_dir, 100)\n",
    "# print(image_dataset.shape)\n",
    "# print(image_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train = createDatasetWithBatches(image_dataset, labels, batch_size = 5, shuffle = True)\n",
    "# dataset_val = dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelInit():\n",
    "    input_shape = (128, 128, 6)\n",
    "    hidden_layer_size, num_classes = 1000, 8\n",
    "    initializer = tf.keras.initializers.VarianceScaling(scale=2.0)\n",
    "    layers = [\n",
    "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu',\n",
    "                              kernel_initializer=initializer),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax', \n",
    "                              kernel_initializer=initializer),\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    return model\n",
    "\n",
    "def OptimizerInit():\n",
    "    learning_rate = 0.001\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel(base_path, files_in_dir, labels_in_dir, batch_size, number_of_training_samples, device, model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False): \n",
    "\n",
    "    with tf.device(device):\n",
    "\n",
    "        loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        model = ModelInit()\n",
    "        optimizer = OptimizerInit()\n",
    "        \n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.MeanSquaredError(name='train_accuracy')\n",
    "    \n",
    "        val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "        val_accuracy = tf.keras.metrics.MeanSquaredError(name='val_accuracy')\n",
    "        \n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics\n",
    "            train_loss.reset_states()\n",
    "            train_accuracy.reset_states()\n",
    "\n",
    "            num_terations_per_epoch = int(number_of_training_samples / batch_size)\n",
    "\n",
    "            for iteration in tqdm(range(num_terations_per_epoch)):\n",
    "            \n",
    "                images_batch, labels_batch = loadDataBatches(base_path, files_in_dir, labels_in_dir, batch_size)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    \n",
    "                    # Use the model function to build the forward pass.\n",
    "                    scores = model(images_batch, training=is_training)\n",
    "                    loss = loss_fn(labels_batch, scores)\n",
    "      \n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                    \n",
    "                    # Update the metrics\n",
    "                    train_loss.update_state(loss)\n",
    "                    train_accuracy.update_state(labels_batch, scores)\n",
    "\n",
    "                    print_every = 2\n",
    "                    if t % print_every == 0:\n",
    "                        val_loss.reset_states()\n",
    "                        val_accuracy.reset_states()\n",
    "                        # for test_x, test_y in val_dset:\n",
    "                        #     # During validation at end of epoch, training set to False\n",
    "                        #     prediction = model(test_x, training=False)\n",
    "                        #     t_loss = loss_fn(test_y, prediction)\n",
    "\n",
    "                        #     val_loss.update_state(t_loss)\n",
    "                        #     val_accuracy.update_state(test_y, prediction)\n",
    "                        \n",
    "                        template = 'Iteration {}, Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}'\n",
    "                        print (template.format(t, epoch+1,\n",
    "                                             train_loss.result().numpy(),\n",
    "                                             train_accuracy.result().numpy(),\n",
    "                                             0,\n",
    "                                             0))\n",
    "                    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "number_of_training_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 10%|█         | 1/10 [00:01<00:14,  1.63s/it]Iteration 0, Epoch 1, Loss: 298.4125061035156, Accuracy: 298.4125061035156, Val Loss: 0, Val Accuracy: 0\n",
      " 30%|███       | 3/10 [00:03<00:07,  1.10s/it]Iteration 2, Epoch 1, Loss: 298.9006042480469, Accuracy: 298.90057373046875, Val Loss: 0, Val Accuracy: 0\n",
      " 50%|█████     | 5/10 [00:05<00:05,  1.02s/it]Iteration 4, Epoch 1, Loss: 298.9753723144531, Accuracy: 298.975341796875, Val Loss: 0, Val Accuracy: 0\n",
      " 70%|███████   | 7/10 [00:07<00:02,  1.01it/s]Iteration 6, Epoch 1, Loss: 299.00738525390625, Accuracy: 299.00738525390625, Val Loss: 0, Val Accuracy: 0\n",
      " 90%|█████████ | 9/10 [00:09<00:00,  1.03it/s]Iteration 8, Epoch 1, Loss: 299.0251770019531, Accuracy: 299.02520751953125, Val Loss: 0, Val Accuracy: 0\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "TrainModel(base_path, files_in_dir, labels_in_dir, batch_size, number_of_training_samples, device, ModelInit, OptimizerInit, num_epochs=1, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}