{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/gokul/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/gokul/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/gokul/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/gokul/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/gokul/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/gokul/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from skimage import data, exposure, img_as_float\n",
    "import matplotlib.pyplot as plt\n",
    "from Misc.MiscUtils import *\n",
    "from Misc.DataUtils import *\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import shutil\n",
    "from io import StringIO\n",
    "import string\n",
    "from termcolor import colored, cprint\n",
    "import math as m\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Don't generate pyc codes\n",
    "sys.dont_write_bytecode = True\n",
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loadData(folder_name, files_in_dir, labels_in_dir, num_training=5000):\n",
    "\n",
    "#     images_data = []\n",
    "#     labels_data = []\n",
    "\n",
    "#     if(len(files_in_dir) < num_training):\n",
    "#         print(\"The data has only \", len(files_in_dir) , \" images and you are trying to get \",num_training, \" images\")\n",
    "#         num_training = len(files_in_dir)\n",
    "\n",
    "#     for n in range(num_training):\n",
    "#         image1_name = folder_name + os.sep + \"Train_synthetic/PA/\" + files_in_dir[n,0]\n",
    "#         image1 = cv2.imread(image1_name)\n",
    "\n",
    "#         image2_name = folder_name + os.sep + \"Train_synthetic/PB/\" + files_in_dir[n,0] \n",
    "#         image2 = cv2.imread(image2_name)\n",
    "\n",
    "#         if(image1 is None) or (image1 is None):\n",
    "#             print(image1_name, \" is empty. Ignoring ...\")\n",
    "#             continue\n",
    "\n",
    "#         image1 = np.float32(image1)\n",
    "#         image2 = np.float32(image2)        \n",
    "\n",
    "\n",
    "#         #combile images along depth\n",
    "\n",
    "#         image = np.dstack((image1, image1))\n",
    "#         # #standardize image from \n",
    "#         # mean = np.mean(image, axis=(1,2), keepdims=True)\n",
    "#         # std = np.std(image, axis=(1,2), keepdims=True)\n",
    "#         # standardized_image = (image - mean) / (std + 0.000001)\n",
    "         \n",
    "\n",
    "#         labels_data.append(labels_in_dir[n,:])\n",
    "\n",
    "#         images_data.append(image)\n",
    "#         # labels_data.append(label)\n",
    "\n",
    "#     return np.array(images_data), np.array(labels_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createDatasetWithBatches(images_data, labels_data, batch_size, shuffle = True):\n",
    "    \n",
    "#     N, B = images_data.shape[0], batch_size\n",
    "#     idx = np.arange(N)\n",
    "#     if shuffle:\n",
    "#         np.random.shuffle(idx)\n",
    "#     return iter((images_data[idx[i:i+B]], labels_data[idx[i:i+B]]) for i in range(0, N, B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataBatches(base_path, files_in_dir, labels_in_dir, batch_size):\n",
    "\n",
    "    images_data = []\n",
    "    labels_data = []\n",
    "\n",
    "    for n in range(batch_size):\n",
    "        random_index = random.randint(0, len(files_in_dir)-1)\n",
    "\n",
    "        image1_name = base_path + os.sep + \"Train_synthetic/PA/\" + files_in_dir[random_index, 0]\n",
    "        image1 = cv2.imread(image1_name)\n",
    "        image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        image2_name = base_path + os.sep + \"Train_synthetic/PB/\" + files_in_dir[random_index, 0] \n",
    "        image2 = cv2.imread(image2_name)\n",
    "        image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        if(image1 is None) or (image1 is None):\n",
    "            print(image1_name, \" is empty. Ignoring ...\")\n",
    "            continue\n",
    "\n",
    "        image1 = np.float32(image1)\n",
    "        image2 = np.float32(image2) \n",
    "\n",
    "        image = np.dstack((image1, image1))\n",
    "        images_data.append(image)\n",
    "\n",
    "        labels_data.append(labels_in_dir[n,:])\n",
    "\n",
    "    images_data = np.array(images_data)\n",
    "    labels_data = np.array(labels_data)\n",
    "\n",
    "    return images_data, labels_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/gokul/CMSC733/hgokul_p1/Phase2/Data\"\n",
    "CheckPointPath = \"/home/gokul/CMSC733/hgokul_p1/Phase2/Checkpoints/supervised/\"\n",
    "files_in_dir, SaveCheckPoint, ImageSize, NumTrainSamples, labels_in_dir = SetupAll(base_path, CheckPointPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "# Constant to control how often we print when training models\n",
    "print_every = 100\n",
    "\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image_dataset, labels = loadData(BasePath, files_in_dir, labels_in_dir, 100)\n",
    "# print(image_dataset.shape)\n",
    "# print(image_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train = createDatasetWithBatches(image_dataset, labels, batch_size = 5, shuffle = True)\n",
    "# dataset_val = dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import  Activation, Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization, InputLayer\n",
    "\n",
    "def ModelInit():\n",
    "\n",
    "    hidden_layer_size, num_classes = 1000, 8\n",
    "    input_shape = (128, 128, 2)\n",
    "    kernel_size = 3\n",
    "    pool_size = 2\n",
    "    filters = 64\n",
    "    dropout = 0.5\n",
    "\n",
    "    initializer = tf.keras.initializers.VarianceScaling(scale=2.0)\n",
    "#     layers = [\n",
    "#         Flatten(input_shape=input_shape),\n",
    "#         Dense(hidden_layer_size, activation='relu',\n",
    "#                               kernel_initializer=initializer),\n",
    "#         Dense(num_classes, activation='softmax', \n",
    "#                               kernel_initializer=initializer),\n",
    "#     ]\n",
    "#     model = Sequential(layers)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape))\n",
    "    ## conv2d 128\n",
    "    model.add(Conv2D(filters=filters,\\\n",
    "      kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    ## conv2d 128\n",
    "    model.add(Conv2D(filters=filters,\\\n",
    "            kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size))\n",
    "    \n",
    "    ## conv2d 64\n",
    "    model.add(Conv2D(filters=filters,\\\n",
    "            kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    ## conv2d 64\n",
    "    model.add(Conv2D(filters=filters,\\\n",
    "            kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size))\n",
    "    \n",
    "    ## conv2d 32 2x Filters\n",
    "    model.add(Conv2D(filters=filters*2,\\\n",
    "            kernel_size=kernel_size, activation='relu', padding='same',))\n",
    "    model.add(BatchNormalization())\n",
    "    ## conv2d 32 2x Filters\n",
    "    model.add(Conv2D(filters=filters*2,\\\n",
    "            kernel_size=kernel_size, activation='relu', padding='same',))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size))\n",
    "    \n",
    "    ## conv2d 16 2x Filters\n",
    "    model.add(Conv2D(filters=filters*2,\\\n",
    "            kernel_size=kernel_size, activation='relu', padding='same',))\n",
    "    model.add(BatchNormalization())\n",
    "    ## conv2d 16 2x Filters\n",
    "    model.add(Conv2D(filters=filters*2,\\\n",
    "            kernel_size=kernel_size, activation='relu', padding='same',))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    #for regression model\n",
    "    model.add(Dense(8))\n",
    "    return model\n",
    "\n",
    "def OptimizerInit():\n",
    "    learning_rate = 0.001\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel(base_path, files_in_dir, labels_in_dir, batch_size, number_of_training_samples, device, model_init_fn, optimizer_init_fn, num_epochs, CheckPointPath, is_training=False): \n",
    "\n",
    "    with tf.device(device):\n",
    "\n",
    "        loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        model = ModelInit()\n",
    "        optimizer = OptimizerInit()\n",
    "        \n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.MeanSquaredError(name='train_accuracy')\n",
    "    \n",
    "        val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "        val_accuracy = tf.keras.metrics.MeanSquaredError(name='val_accuracy')\n",
    "        \n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics\n",
    "            train_loss.reset_states()\n",
    "            train_accuracy.reset_states()\n",
    "\n",
    "            num_terations_per_epoch = int(number_of_training_samples / batch_size)\n",
    "\n",
    "            tf.keras.callbacks.ModelCheckpoint(CheckPointPath, monitor='val_loss', verbose=0, save_best_only=False,\n",
    "            save_weights_only=False, mode='auto')\n",
    "\n",
    "            for iteration in tqdm(range(num_terations_per_epoch)):\n",
    "            \n",
    "                images_batch, labels_batch = loadDataBatches(base_path, files_in_dir, labels_in_dir, batch_size)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    \n",
    "                    # Use the model function to build the forward pass.\n",
    "                    scores = model(images_batch, training=is_training)\n",
    "                    loss = loss_fn(labels_batch, scores)\n",
    "      \n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                    \n",
    "                    # Update the metrics\n",
    "                    train_loss.update_state(loss)\n",
    "                    train_accuracy.update_state(labels_batch, scores)                    \n",
    "                    print_every = 10\n",
    "                    if t % print_every == 0:\n",
    "                        val_loss.reset_states()\n",
    "                        val_accuracy.reset_states()\n",
    "\n",
    "                        # for test_x, test_y in val_dset:\n",
    "                        #     # During validation at end of epoch, training set to False\n",
    "                        #     prediction = model(test_x, training=False)\n",
    "                        #     t_loss = loss_fn(test_y, prediction)\n",
    "\n",
    "                        #     val_loss.update_state(t_loss)\n",
    "                        #     val_accuracy.update_state(test_y, prediction)\n",
    "                        \n",
    "                        template = 'Iteration {}, Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}'\n",
    "                        print (template.format(t, epoch+1,\n",
    "                                             train_loss.result().numpy(),\n",
    "                                             train_accuracy.result().numpy(),\n",
    "                                             0,\n",
    "                                             0))\n",
    "                    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "number_of_training_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gokul/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:58: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gokul/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:442: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gokul/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3543: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gokul/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3386: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/gokul/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1205: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/gokul/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2888: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer sequential_1 was called with an input that isn't a symbolic tensor. Received type: <class 'numpy.ndarray'>. Full input: [array([[[[ 62.,  62.],\n         [ 42.,  42.],\n         [ 28.,  28.],\n         ...,\n         [127., 127.],\n         [130., 130.],\n         [133., 133.]],\n\n        [[ 40.,  40.],\n         [ 34.,  34.],\n         [ 34.,  34.],\n         ...,\n         [133., 133.],\n         [130., 130.],\n         [133., 133.]],\n\n        [[ 43.,  43.],\n         [ 32.,  32.],\n         [ 40.,  40.],\n         ...,\n         [119., 119.],\n         [114., 114.],\n         [135., 135.]],\n\n        ...,\n\n        [[108., 108.],\n         [109., 109.],\n         [110., 110.],\n         ...,\n         [101., 101.],\n         [ 97.,  97.],\n         [ 97.,  97.]],\n\n        [[108., 108.],\n         [109., 109.],\n         [108., 108.],\n         ...,\n         [ 95.,  95.],\n         [102., 102.],\n         [100., 100.]],\n\n        [[109., 109.],\n         [108., 108.],\n         [110., 110.],\n         ...,\n         [ 95.,  95.],\n         [ 96.,  96.],\n         [ 95.,  95.]]],\n\n\n       [[[ 92.,  92.],\n         [109., 109.],\n         [103., 103.],\n         ...,\n         [167., 167.],\n         [173., 173.],\n         [166., 166.]],\n\n        [[103., 103.],\n         [108., 108.],\n         [107., 107.],\n         ...,\n         [170., 170.],\n         [159., 159.],\n         [170., 170.]],\n\n        [[109., 109.],\n         [116., 116.],\n         [ 97.,  97.],\n         ...,\n         [158., 158.],\n         [173., 173.],\n         [154., 154.]],\n\n        ...,\n\n        [[190., 190.],\n         [192., 192.],\n         [189., 189.],\n         ...,\n         [ 70.,  70.],\n         [ 72.,  72.],\n         [ 73.,  73.]],\n\n        [[185., 185.],\n         [190., 190.],\n         [190., 190.],\n         ...,\n         [ 70.,  70.],\n         [ 71.,  71.],\n         [ 72.,  72.]],\n\n        [[170., 170.],\n         [185., 185.],\n         [191., 191.],\n         ...,\n         [ 68.,  68.],\n         [ 70.,  70.],\n         [ 71.,  71.]]],\n\n\n       [[[  0.,   0.],\n         [  0.,   0.],\n         [  0.,   0.],\n         ...,\n         [  3.,   3.],\n         [  2.,   2.],\n         [  0.,   0.]],\n\n        [[  1.,   1.],\n         [  0.,   0.],\n         [  0.,   0.],\n         ...,\n         [  0.,   0.],\n         [  2.,   2.],\n         [  5.,   5.]],\n\n        [[  0.,   0.],\n         [  0.,   0.],\n         [  0.,   0.],\n         ...,\n         [  3.,   3.],\n         [  0.,   0.],\n         [  0.,   0.]],\n\n        ...,\n\n        [[244., 244.],\n         [254., 254.],\n         [254., 254.],\n         ...,\n         [192., 192.],\n         [ 89.,  89.],\n         [ 75.,  75.]],\n\n        [[132., 132.],\n         [192., 192.],\n         [255., 255.],\n         ...,\n         [130., 130.],\n         [234., 234.],\n         [255., 255.]],\n\n        [[177., 177.],\n         [202., 202.],\n         [253., 253.],\n         ...,\n         [ 43.,  43.],\n         [ 56.,  56.],\n         [185., 185.]]],\n\n\n       ...,\n\n\n       [[[ 30.,  30.],\n         [ 25.,  25.],\n         [ 48.,  48.],\n         ...,\n         [ 40.,  40.],\n         [ 52.,  52.],\n         [ 50.,  50.]],\n\n        [[ 32.,  32.],\n         [ 36.,  36.],\n         [ 50.,  50.],\n         ...,\n         [ 37.,  37.],\n         [ 59.,  59.],\n         [ 57.,  57.]],\n\n        [[ 49.,  49.],\n         [ 61.,  61.],\n         [ 51.,  51.],\n         ...,\n         [ 39.,  39.],\n         [ 52.,  52.],\n         [ 59.,  59.]],\n\n        ...,\n\n        [[ 56.,  56.],\n         [ 34.,  34.],\n         [ 22.,  22.],\n         ...,\n         [ 18.,  18.],\n         [ 23.,  23.],\n         [ 47.,  47.]],\n\n        [[ 48.,  48.],\n         [ 37.,  37.],\n         [ 26.,  26.],\n         ...,\n         [ 34.,  34.],\n         [ 27.,  27.],\n         [ 33.,  33.]],\n\n        [[ 31.,  31.],\n         [ 30.,  30.],\n         [ 30.,  30.],\n         ...,\n         [ 54.,  54.],\n         [ 43.,  43.],\n         [ 36.,  36.]]],\n\n\n       [[[137., 137.],\n         [148., 148.],\n         [129., 129.],\n         ...,\n         [126., 126.],\n         [169., 169.],\n         [192., 192.]],\n\n        [[146., 146.],\n         [145., 145.],\n         [138., 138.],\n         ...,\n         [ 80.,  80.],\n         [137., 137.],\n         [150., 150.]],\n\n        [[148., 148.],\n         [148., 148.],\n         [143., 143.],\n         ...,\n         [ 30.,  30.],\n         [ 68.,  68.],\n         [114., 114.]],\n\n        ...,\n\n        [[ 98.,  98.],\n         [ 99.,  99.],\n         [ 95.,  95.],\n         ...,\n         [108., 108.],\n         [105., 105.],\n         [106., 106.]],\n\n        [[ 97.,  97.],\n         [ 96.,  96.],\n         [ 95.,  95.],\n         ...,\n         [113., 113.],\n         [111., 111.],\n         [112., 112.]],\n\n        [[ 98.,  98.],\n         [ 92.,  92.],\n         [ 95.,  95.],\n         ...,\n         [128., 128.],\n         [127., 127.],\n         [119., 119.]]],\n\n\n       [[[164., 164.],\n         [158., 158.],\n         [161., 161.],\n         ...,\n         [160., 160.],\n         [160., 160.],\n         [160., 160.]],\n\n        [[158., 158.],\n         [163., 163.],\n         [161., 161.],\n         ...,\n         [129., 129.],\n         [112., 112.],\n         [ 94.,  94.]],\n\n        [[162., 162.],\n         [162., 162.],\n         [161., 161.],\n         ...,\n         [ 39.,  39.],\n         [ 43.,  43.],\n         [ 43.,  43.]],\n\n        ...,\n\n        [[ 65.,  65.],\n         [ 63.,  63.],\n         [ 65.,  65.],\n         ...,\n         [ 54.,  54.],\n         [ 69.,  69.],\n         [ 67.,  67.]],\n\n        [[ 66.,  66.],\n         [ 56.,  56.],\n         [ 51.,  51.],\n         ...,\n         [ 57.,  57.],\n         [ 68.,  68.],\n         [ 62.,  62.]],\n\n        [[ 57.,  57.],\n         [ 52.,  52.],\n         [ 64.,  64.],\n         ...,\n         [ 52.,  52.],\n         [ 63.,  63.],\n         [ 66.,  66.]]]], dtype=float32)]. All inputs to the layer should be tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    402\u001b[0m                           tf.SparseTensor)):\n\u001b[0;32m--> 403\u001b[0;31m         raise ValueError('Unexpectedly found an instance of type `' + str(type(x)) + '`. '\n\u001b[0m\u001b[1;32m    404\u001b[0m                          'Expected a symbolic tensor instance.')\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpectedly found an instance of type `<class 'numpy.ndarray'>`. Expected a symbolic tensor instance.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-22974aa51e5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles_in_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_in_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_training_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelInit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptimizerInit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCheckPointPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCheckPointPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-5dd60c117d04>\u001b[0m in \u001b[0;36mTrainModel\u001b[0;34m(base_path, files_in_dir, labels_in_dir, batch_size, number_of_training_samples, device, model_init_fn, optimizer_init_fn, num_epochs, CheckPointPath, is_training)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0;31m# Use the model function to build the forward pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                  \u001b[0;34m'Received type: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full input: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. All inputs to the layer '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                                  'should be tensors.')\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer sequential_1 was called with an input that isn't a symbolic tensor. Received type: <class 'numpy.ndarray'>. Full input: [array([[[[ 62.,  62.],\n         [ 42.,  42.],\n         [ 28.,  28.],\n         ...,\n         [127., 127.],\n         [130., 130.],\n         [133., 133.]],\n\n        [[ 40.,  40.],\n         [ 34.,  34.],\n         [ 34.,  34.],\n         ...,\n         [133., 133.],\n         [130., 130.],\n         [133., 133.]],\n\n        [[ 43.,  43.],\n         [ 32.,  32.],\n         [ 40.,  40.],\n         ...,\n         [119., 119.],\n         [114., 114.],\n         [135., 135.]],\n\n        ...,\n\n        [[108., 108.],\n         [109., 109.],\n         [110., 110.],\n         ...,\n         [101., 101.],\n         [ 97.,  97.],\n         [ 97.,  97.]],\n\n        [[108., 108.],\n         [109., 109.],\n         [108., 108.],\n         ...,\n         [ 95.,  95.],\n         [102., 102.],\n         [100., 100.]],\n\n        [[109., 109.],\n         [108., 108.],\n         [110., 110.],\n         ...,\n         [ 95.,  95.],\n         [ 96.,  96.],\n         [ 95.,  95.]]],\n\n\n       [[[ 92.,  92.],\n         [109., 109.],\n         [103., 103.],\n         ...,\n         [167., 167.],\n         [173., 173.],\n         [166., 166.]],\n\n        [[103., 103.],\n         [108., 108.],\n         [107., 107.],\n         ...,\n         [170., 170.],\n         [159., 159.],\n         [170., 170.]],\n\n        [[109., 109.],\n         [116., 116.],\n         [ 97.,  97.],\n         ...,\n         [158., 158.],\n         [173., 173.],\n         [154., 154.]],\n\n        ...,\n\n        [[190., 190.],\n         [192., 192.],\n         [189., 189.],\n         ...,\n         [ 70.,  70.],\n         [ 72.,  72.],\n         [ 73.,  73.]],\n\n        [[185., 185.],\n         [190., 190.],\n         [190., 190.],\n         ...,\n         [ 70.,  70.],\n         [ 71.,  71.],\n         [ 72.,  72.]],\n\n        [[170., 170.],\n         [185., 185.],\n         [191., 191.],\n         ...,\n         [ 68.,  68.],\n         [ 70.,  70.],\n         [ 71.,  71.]]],\n\n\n       [[[  0.,   0.],\n         [  0.,   0.],\n         [  0.,   0.],\n         ...,\n         [  3.,   3.],\n         [  2.,   2.],\n         [  0.,   0.]],\n\n        [[  1.,   1.],\n         [  0.,   0.],\n         [  0.,   0.],\n         ...,\n         [  0.,   0.],\n         [  2.,   2.],\n         [  5.,   5.]],\n\n        [[  0.,   0.],\n         [  0.,   0.],\n         [  0.,   0.],\n         ...,\n         [  3.,   3.],\n         [  0.,   0.],\n         [  0.,   0.]],\n\n        ...,\n\n        [[244., 244.],\n         [254., 254.],\n         [254., 254.],\n         ...,\n         [192., 192.],\n         [ 89.,  89.],\n         [ 75.,  75.]],\n\n        [[132., 132.],\n         [192., 192.],\n         [255., 255.],\n         ...,\n         [130., 130.],\n         [234., 234.],\n         [255., 255.]],\n\n        [[177., 177.],\n         [202., 202.],\n         [253., 253.],\n         ...,\n         [ 43.,  43.],\n         [ 56.,  56.],\n         [185., 185.]]],\n\n\n       ...,\n\n\n       [[[ 30.,  30.],\n         [ 25.,  25.],\n         [ 48.,  48.],\n         ...,\n         [ 40.,  40.],\n         [ 52.,  52.],\n         [ 50.,  50.]],\n\n        [[ 32.,  32.],\n         [ 36.,  36.],\n         [ 50.,  50.],\n         ...,\n         [ 37.,  37.],\n         [ 59.,  59.],\n         [ 57.,  57.]],\n\n        [[ 49.,  49.],\n         [ 61.,  61.],\n         [ 51.,  51.],\n         ...,\n         [ 39.,  39.],\n         [ 52.,  52.],\n         [ 59.,  59.]],\n\n        ...,\n\n        [[ 56.,  56.],\n         [ 34.,  34.],\n         [ 22.,  22.],\n         ...,\n         [ 18.,  18.],\n         [ 23.,  23.],\n         [ 47.,  47.]],\n\n        [[ 48.,  48.],\n         [ 37.,  37.],\n         [ 26.,  26.],\n         ...,\n         [ 34.,  34.],\n         [ 27.,  27.],\n         [ 33.,  33.]],\n\n        [[ 31.,  31.],\n         [ 30.,  30.],\n         [ 30.,  30.],\n         ...,\n         [ 54.,  54.],\n         [ 43.,  43.],\n         [ 36.,  36.]]],\n\n\n       [[[137., 137.],\n         [148., 148.],\n         [129., 129.],\n         ...,\n         [126., 126.],\n         [169., 169.],\n         [192., 192.]],\n\n        [[146., 146.],\n         [145., 145.],\n         [138., 138.],\n         ...,\n         [ 80.,  80.],\n         [137., 137.],\n         [150., 150.]],\n\n        [[148., 148.],\n         [148., 148.],\n         [143., 143.],\n         ...,\n         [ 30.,  30.],\n         [ 68.,  68.],\n         [114., 114.]],\n\n        ...,\n\n        [[ 98.,  98.],\n         [ 99.,  99.],\n         [ 95.,  95.],\n         ...,\n         [108., 108.],\n         [105., 105.],\n         [106., 106.]],\n\n        [[ 97.,  97.],\n         [ 96.,  96.],\n         [ 95.,  95.],\n         ...,\n         [113., 113.],\n         [111., 111.],\n         [112., 112.]],\n\n        [[ 98.,  98.],\n         [ 92.,  92.],\n         [ 95.,  95.],\n         ...,\n         [128., 128.],\n         [127., 127.],\n         [119., 119.]]],\n\n\n       [[[164., 164.],\n         [158., 158.],\n         [161., 161.],\n         ...,\n         [160., 160.],\n         [160., 160.],\n         [160., 160.]],\n\n        [[158., 158.],\n         [163., 163.],\n         [161., 161.],\n         ...,\n         [129., 129.],\n         [112., 112.],\n         [ 94.,  94.]],\n\n        [[162., 162.],\n         [162., 162.],\n         [161., 161.],\n         ...,\n         [ 39.,  39.],\n         [ 43.,  43.],\n         [ 43.,  43.]],\n\n        ...,\n\n        [[ 65.,  65.],\n         [ 63.,  63.],\n         [ 65.,  65.],\n         ...,\n         [ 54.,  54.],\n         [ 69.,  69.],\n         [ 67.,  67.]],\n\n        [[ 66.,  66.],\n         [ 56.,  56.],\n         [ 51.,  51.],\n         ...,\n         [ 57.,  57.],\n         [ 68.,  68.],\n         [ 62.,  62.]],\n\n        [[ 57.,  57.],\n         [ 52.,  52.],\n         [ 64.,  64.],\n         ...,\n         [ 52.,  52.],\n         [ 63.,  63.],\n         [ 66.,  66.]]]], dtype=float32)]. All inputs to the layer should be tensors."
     ]
    }
   ],
   "source": [
    "TrainModel(base_path, files_in_dir, labels_in_dir, batch_size, number_of_training_samples, device, ModelInit, OptimizerInit, num_epochs=5, CheckPointPath = CheckPointPath, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
