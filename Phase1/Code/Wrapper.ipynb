{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/evn python\n",
    "\n",
    "\"\"\"\n",
    "CMSC733 Spring 2019: Classical and Deep Learning Approaches for\n",
    "Geometric Computer Vision\n",
    "Project1: MyAutoPano: Phase 1 Starter Code\n",
    "\n",
    "Author(s): \n",
    "Chahat Deep Singh (chahat@terpmail.umd.edu) \n",
    "PhD Student in Computer Science,\n",
    "University of Maryland, College Park\n",
    "\n",
    "Nitin J. Sanket (nitinsan@terpmail.umd.edu)\n",
    "PhD Candidate in Computer Science,\n",
    "University of Maryland, College Park\n",
    "\"\"\"\n",
    "\n",
    "#Code starts here:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from skimage.feature import peak_local_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readImageSet(folder_name):\n",
    "    print(\"Reading images from \", folder_name)\n",
    "    images = []\n",
    "    files = os.listdir(folder_name)\t\n",
    "    files = sorted(files)\n",
    "    print(\"Found \", files)\n",
    "    for file in files:\n",
    "        image_path = folder_name + \"/\" + file\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is not None:\n",
    "            images.append(image)\t\t\t\n",
    "        else:\n",
    "            print(\"Error in loading image \", image)\n",
    "\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayImages(image_array, name):\n",
    "\n",
    "    image_array = makeImageSizeSame(image_array)\n",
    "    concat = image_array[0].copy()\n",
    "\n",
    "    for l in range(1,len(image_array)):\n",
    "        image = image_array[l]\n",
    "        concat = np.concatenate((concat,image), axis = 1)\n",
    "        \n",
    "    cv2.imshow(name, concat)\n",
    "    cv2.waitKey() \n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectCorners(images, choice):\n",
    "    print(\"detecting corners ...\")\n",
    "    detected_corners = []\n",
    "    cmaps = []\n",
    "    corner_images = []\n",
    "    for i in images:\n",
    "        image = i.copy()\n",
    "        gray_image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        gray_image = np.float32(gray_image)\n",
    "\n",
    "\n",
    "        if(choice == 1):\n",
    "            print(\"using Harris corner detection method.\")\n",
    "            corner_strength = cv2.cornerHarris(gray_image,2,3,0.04)\n",
    "            corner_strength[corner_strength<0.01*corner_strength.max()] = 0\n",
    "            detected_corner = np.where(corner_strength>0.001*corner_strength.max())\n",
    "            detected_corners.append(detected_corner)\n",
    "            cmaps.append(corner_strength)\n",
    "            image[corner_strength > 0.001*corner_strength.max()]=[0,0,255]\n",
    "            corner_images.append(image)\n",
    "        else:\n",
    "            print(\"using Shi-Tomashi corner detection method.\")\n",
    "            dst = cv2.goodFeaturesToTrack(gray_image, 1000 ,0.01, 10)\n",
    "            dst = np.int0(dst)\n",
    "            detected_corners.append(dst)\n",
    "            for c in dst:\n",
    "                x,y = c.ravel()\n",
    "                cv2.circle(image,(x,y),3,(0, 0, 255),-1) \n",
    "                          \n",
    "            corner_images.append(image)\n",
    "            cmap = np.zeros(gray_image.shape) #not sure what to do\n",
    "            cmaps.append(cmap)\n",
    "    #filter detected corners\n",
    "    #remove the corner one\n",
    "    return detected_corners, cmaps, corner_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureDescriptor(gray_image,x,y, patch_size=40):\n",
    "    patch = gray_image[x-patch_size//2:x+patch_size//2, y-patch_size//2:y+patch_size//2] \n",
    "    # gaussian blur\n",
    "    patch = cv2.GaussianBlur(patch,(3,3),0)\n",
    "    # subsample to 20% size or 1/5th\n",
    "    patch = cv2.resize(patch, None, fx=0.2, fy=0.2, interpolation = cv2.INTER_CUBIC)\n",
    "    feature = patch.reshape(-1)\n",
    "    feature = (feature-feature.mean())/ np.std(feature)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPairs(image_1, image_2, all_corners_1, all_corners_2, patch_size = 40, alpha = 0.8):\n",
    "\n",
    "    gray_image1 = cv2.cvtColor(image_1,cv2.COLOR_BGR2GRAY)\n",
    "    gray_image2 = cv2.cvtColor(image_2,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    width1, height1 = image_1.shape[:2]\n",
    "    width2, height2 = image_2.shape[:2]\n",
    "\n",
    "    print(\"width = \", width1, \", height = \", height1)\n",
    "    print(\"width = \", width2, \", height = \", height2)\n",
    "\n",
    "    features_1,features_2 = [], []\n",
    "    corners_1, corners_2 = [],[]\n",
    "\n",
    "    print(\"all corner 1 len = \", len(all_corners_1))\n",
    "    print(\"all corner 2 len = \", len(all_corners_2))\n",
    "    \n",
    "    for corner in all_corners_1:\n",
    "        x,y = corner.ravel()\n",
    "        \n",
    "        if (x - (patch_size / 2) > 0) & (x + (patch_size / 2) < height1) & (y - (patch_size / 2) > 0) & (y + (patch_size / 2) < width1):\n",
    "            features_1.append(getFeatureDescriptor(gray_image1, y,x)) \n",
    "            corners_1.append([x,y])\n",
    "        else:\n",
    "            #print(\"ignored x, y\", x, y)\n",
    "            pass\n",
    "\n",
    "    for corner in all_corners_2:\n",
    "        x,y = corner.ravel()\n",
    "        if (x - (patch_size / 2) > 0) & (x + (patch_size / 2) < height2) & (y - (patch_size / 2) > 0) & (y + (patch_size / 2) < width2):\n",
    "            features_2.append(getFeatureDescriptor(gray_image2, y,x)) \n",
    "            corners_2.append([x,y]) \n",
    "\n",
    "    matched_pairs, match_level = [], []\n",
    "    for i, feat_1 in enumerate(features_1):\n",
    "        ssd = []  \n",
    "        for j, feat_2 in enumerate(features_2):\n",
    "            ssd.append(np.sum((feat_1 - feat_2)**2))\n",
    "        top_matche = np.argmin(ssd)\n",
    "        #if ssd[top_matches[0]] / ssd[top_matches[1]] < alpha:   \n",
    "            #matched_pairs.append([corners_1[i] , corners_2[top_matches[0]]])\n",
    "        matched_pairs.append([corners_1[i] , corners_2[top_matche]]) \n",
    "    print(\"matched pairs num = \", len(matched_pairs))\n",
    "    matched_pairs = np.array(matched_pairs)\n",
    "    return matched_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeImageSizeSame(images):\n",
    "    sizes = []\n",
    "    for image in images:\n",
    "        x, y, ch = image.shape\n",
    "        sizes.append([x, y, ch])\n",
    "\n",
    "    sizes = np.array(sizes)\n",
    "    x_target, y_target, _ = np.max(sizes, axis = 0)\n",
    "    \n",
    "    images_resized = []\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        image_resized = np.zeros((x_target, y_target, sizes[i, 2]), np.uint8)\n",
    "        image_resized[0:sizes[i, 0], 0:sizes[i, 1], 0:sizes[i, 2]] = image\n",
    "        images_resized.append(image_resized)\n",
    "\n",
    "    return images_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showMatches(image_1, image_2, matched_pairs, partition_width = 20):\n",
    "\n",
    "    image_1, image_2 = makeImageSizeSame([image_1, image_2])\n",
    "\n",
    "    concat = np.concatenate((image_1, image_2), axis = 1)\n",
    "    corners_1 = matched_pairs[:,0].copy()\n",
    "    corners_2  = matched_pairs[:,1].copy()\n",
    "    corners_2[:,0] += image_1.shape[1]\n",
    "\n",
    "    for (x1,y1) , (x2,y2) in zip(corners_1, corners_2):\n",
    "        cv2.line(concat, (x1,y1), (x2,y2), (0, 0, 255), 1)\n",
    "    \n",
    "      \n",
    "    cv2.imshow('Feature_matches', concat)\n",
    "    cv2.waitKey() \n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testShowMatches(image_1, image_2, partition_width = 20):\n",
    "    matched_pairs= []\n",
    "    I = np.linspace(10, 100, 10)\n",
    "    for i in I:\n",
    "        x1 = i\n",
    "        y1 = i\n",
    "        corner1 = np.int0(np.array([x1, y1]))\n",
    "\n",
    "        x2 = i\n",
    "        y2 = i\n",
    "        corner2 = np.int0(np.array([x2, y2]))\n",
    "        matched_pairs.append([corner1 , corner2])\n",
    "\n",
    "    matched_pairs = np.array(matched_pairs)\n",
    "    showMatches(image_1, image_2, matched_pairs, partition_width = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterOutliers(matched_pairs, outliers, accuracy, thresh):\n",
    "\n",
    "    set1 = matched_pairs[:, 0]\n",
    "    set2 = matched_pairs[:, 1]\n",
    "\n",
    "    N_best = 0\n",
    "    H_best = np.zeros([3, 3])\n",
    "    \n",
    "    e = outliers / set1.shape[0]\n",
    "    s = 4\n",
    "    p = accuracy\n",
    "    iterations = np.log(1 - p) / np.log(1 - np.power((1 - e), s))\n",
    "    iterations = np.int(iterations)\n",
    "    iterations = 4000\n",
    "\n",
    "    filtered_pair_indices = []\n",
    "\n",
    "    print(\"iterations = \", iterations)\n",
    "    for i in range(iterations):\n",
    "        #randomly select four points\n",
    "        n_rows = set1.shape[0]\n",
    "        random_indices = np.random.choice(n_rows, size=4)\n",
    "\n",
    "        set1_random = set1[random_indices]\n",
    "        set2_random = set2[random_indices]\n",
    "              \n",
    "        #compute homography\n",
    "        H = cv2.getPerspectiveTransform(np.float32(set1_random), np.float32(set2_random))\n",
    "\n",
    "        set1_dash = np.vstack((set1[:,0], set1[:,1], np.ones([1, n_rows])))\n",
    "        set1_transformed_dash = np.dot(H, set1_dash)\n",
    "        \n",
    "        t1 = set1_transformed_dash[0,:]/set1_transformed_dash[2,:]\n",
    "        t2 = set1_transformed_dash[1,:]/set1_transformed_dash[2,:]\n",
    "\n",
    "        set1_transformed = np.array([t1, t2]).T\n",
    "        #print(set1_transformed.shape)\n",
    "\n",
    "        E = calculateError(set2, set1_transformed)\n",
    "        \n",
    "     \n",
    "        E[E <= thresh] = 1\n",
    "        E[E > thresh] = 0\n",
    "    \n",
    "    \n",
    "        N = np.sum(E)\n",
    "\n",
    "\n",
    "        if N > N_best:\n",
    "            N_best = N\n",
    "            H_best = H\n",
    "            filtered_pair_indices = np.where(E == 1)\n",
    "    \n",
    "    filtered_set1 =  set1[filtered_pair_indices]\n",
    "    filtered_set2 =  set2[filtered_pair_indices]\n",
    "\n",
    "    print(\"Number of pairs after filtering = \", filtered_set1.shape[0])\n",
    "\n",
    "    filter_matched_pairs = np.zeros([filtered_set1.shape[0], filtered_set1.shape[1], 2])\n",
    "\n",
    "    filter_matched_pairs[:, 0, :] = filtered_set1\n",
    "    filter_matched_pairs[:, 1, :] = filtered_set2\n",
    "\n",
    "    filter_matched_pairs = filter_matched_pairs.astype(int)\n",
    "\n",
    "    H_inbuit, _ = cv2.findHomography(set1, set2)\n",
    "    print(\"inbuilt = \", H_inbuit)\n",
    "    print(\"Computed = \", H_best)\n",
    "\n",
    "    return H_best, filter_matched_pairs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateError(set1, set2):\n",
    "   \n",
    "    E = np.zeros(set1.shape[0])\n",
    "    tmp = set2 - set1\n",
    "    num = set1.shape[0]\n",
    "\n",
    "    for n in range(num):\n",
    "        E[n] = np.linalg.norm(tmp[n])\n",
    "    return E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaptiveNonMaximalSuppression(images, C_maps, N_best):\n",
    "\n",
    "    anms_img = []\n",
    "    anms_corners = []\n",
    "    for i,image in enumerate(images):\n",
    "\n",
    "        cmap = C_maps[i]\n",
    "        local_maximas = peak_local_max(cmap, min_distance=10)\n",
    "        n_strong = local_maximas.shape[0]\n",
    "        \n",
    "        r = [np.Infinity for i in range(n_strong)]\n",
    "        x=np.zeros((n_strong,1))\n",
    "        y=np.zeros((n_strong,1))\n",
    "        eu_dist = 0\n",
    "\n",
    "        for i in range(n_strong):\n",
    "            for j in range(n_strong):\n",
    "                x_j = local_maximas[j][0]\n",
    "                y_j = local_maximas[j][1]\n",
    "\n",
    "                x_i = local_maximas[i][0]\n",
    "                y_i = local_maximas[i][1]\n",
    "\n",
    "                if(cmap[x_j, y_j] > cmap[x_i, y_i]):\n",
    "                    eu_dist = np.square(x_j - x_i) + np.square(y_j - y_i)\n",
    "                if r[i] > eu_dist:\n",
    "                    r[i] = eu_dist\n",
    "                    x[i] = x_j\n",
    "                    y[i] = y_j\n",
    "\n",
    "        index = np.argsort(r)\n",
    "        index = np.flip(index)\n",
    "        index = index[0:N_best]\n",
    "        x_best=np.zeros((N_best,1))\n",
    "        y_best=np.zeros((N_best,1))\n",
    "\n",
    "        for i in range(N_best):\n",
    "            x_best[i] = np.int0(x[index[i]])\n",
    "            y_best[i] = np.int0(y[index[i]]) \n",
    "            cv2.circle(image, (y_best[i], x_best[i]), 3, (0, 255, 0), -1)\n",
    "\n",
    "        anms_corner = np.int0(np.concatenate((x_best, y_best), axis = 1))\n",
    "        anms_corners.append(anms_corner)\n",
    "        anms_img.append(image)\n",
    "    return anms_corners, anms_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitchImagePairs(img0, img1, H):\n",
    "\n",
    "    image0 = img0.copy()\n",
    "    image1 = img1.copy()\n",
    "\n",
    "    #stitch image 0 on image 1\n",
    "    print(\"shapes\")\n",
    "    print(image0.shape)\n",
    "    print(image1.shape)\n",
    "    \n",
    "\n",
    "    h0 ,w0 ,_ = image0.shape\n",
    "    h1 ,w1 ,_ = image1.shape\n",
    "\n",
    "    points_on_image0 = np.float32([[0, 0], [0, h0], [w0, h0], [w0, 0]]).reshape(-1,1,2)\n",
    "    points_on_image0_transformed = cv2.perspectiveTransform(points_on_image0, H)\n",
    "    print(\"transformed points = \", points_on_image0_transformed)\n",
    "    points_on_image1 = np.float32([[0, 0], [0, h1], [w1, h1], [w1, 0]]).reshape(-1,1,2)\n",
    "\n",
    "    points_on_merged_images = np.concatenate((points_on_image0_transformed, points_on_image1), axis = 0)\n",
    "    points_on_merged_images_ = []\n",
    "\n",
    "    for p in range(len(points_on_merged_images)):\n",
    "        points_on_merged_images_.append(points_on_merged_images[p].ravel())\n",
    "\n",
    "    points_on_merged_images_ = np.array(points_on_merged_images_)\n",
    "\n",
    "    x_min, y_min = np.int0(np.min(points_on_merged_images_, axis = 0))\n",
    "    x_max, y_max = np.int0(np.max(points_on_merged_images_, axis = 0))\n",
    "\n",
    "    print(\"min, max\")\n",
    "    print(x_min, y_min)\n",
    "    print(x_max, y_max)\n",
    "\n",
    "    # overlap_area = cv2.polylines(image1,[np.int32(points_on_image0_transformed)],True,255,3, cv2.LINE_AA) \n",
    "    # cv2.imshow(\"original_image_overlapping.jpg\", overlap_area)\n",
    "    # cv2.waitKey() \n",
    "    # cv2.destroyAllWindows()\n",
    "    H_translate = np.array([[1, 0, -x_min], [0, 1, -y_min], [0, 0, 1]]) # translate\n",
    "\n",
    "    image0_transformed_and_stitched = cv2.warpPerspective(image0, np.dot(H_translate, H), (x_max-x_min, y_max-y_min))\n",
    "    image0_transformed_and_stitched[-y_min:-y_min+h1, -x_min: -x_min+w1] = image1\n",
    "    return image0_transformed_and_stitched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransformImage(image, H): \n",
    "\n",
    "    image0 = image.copy()\n",
    "    h0 ,w0 ,_ = image0.shape\n",
    "\n",
    "    points_on_image0 = np.float32([[0, 0], [0, h0], [w0, h0], [w0, 0]]).reshape(-1,1,2)\n",
    "    points_on_image0_transformed = cv2.perspectiveTransform(points_on_image0, H)\n",
    "\n",
    "    for p in range(len(points_on_image0_transformed)):\n",
    "        points_on_image0_transformed_.append(points_on_image0_transformed[p].ravel())\n",
    "\n",
    "    points_on_image0_transformed_ = np.array(points_on_image0_transformed_)\n",
    "\n",
    "    x_min, y_min = np.int0(np.min(points_on_image0_transformed, axis = 0))\n",
    "    x_max, y_max = np.int0(np.max(points_on_image0_transformed, axis = 0))\n",
    "    \n",
    "    H_translate = np.array([[1, 0, -x_min], [0, 1, -y_min], [0, 0, 1]]) # translate\n",
    "\n",
    "    image0_transformed = cv2.warpPerspective(image0, np.dot(H_translate, H), (x_max-x_min, y_max-y_min))\n",
    "    image0_transformed[-y_min:-y_min+h1, -x_min: -x_min+w1] = image1\n",
    "    return image0_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitchImageArray(image_array, H_array):\n",
    "    \n",
    "    N = len(image_array)\n",
    "\n",
    "    target_image = image_array[N-1]\n",
    "\n",
    "    ht, wt, _ = target_image.shape\n",
    "    points_on_target_image = np.float32([[0, 0], [0, ht], [wt, ht], [wt, 0]]).reshape(-1,1,2)\n",
    "\n",
    "    points = points_on_target_image\n",
    "    \n",
    "    images_transformed = []\n",
    "    H_translate_array = []\n",
    "\n",
    "    for n in range(N-1):\n",
    "        image = image_array[n]\n",
    "        h, w, _ = image.shape\n",
    "        points_on_image = np.float32([[0, 0], [0, h], [w, h], [w, 0]]).reshape(-1,1,2)\n",
    "\n",
    "        points_on_image_transformed = points_on_image \n",
    "        for m in range(n, N-1):\n",
    "            H = H_array[m]\n",
    "            points_on_image_transformed = cv2.perspectiveTransform(points_on_image_transformed, H)\n",
    "        \n",
    "        points = np.concatenate((points, points_on_image_transformed), axis = 0)\n",
    "\n",
    "    points_ = []\n",
    "    for p in range(len(points)):\n",
    "        points_.append(points[p].ravel())\n",
    "\n",
    "    points_ = np.array(points_)\n",
    "\n",
    "    x_min, y_min = np.int0(np.min(points_, axis = 0))\n",
    "    x_max, y_max = np.int0(np.max(points_, axis = 0))\n",
    "\n",
    "    H_translate = np.array([[1, 0, -x_min], [0, 1, -y_min], [0, 0, 1]]) # translate\n",
    "\n",
    "    for n in range(N-1):\n",
    "        image = image_array[n]\n",
    "        image_transformed = image\n",
    "        for m in range(n, N-1):\n",
    "            H = H_array[m]\n",
    "            image_transformed = cv2.warpPerspective(image_transformed, np.dot(H_translate,H), (x_max-x_min, y_max-y_min))\n",
    "\n",
    "        #image_transformed = cv2.warpAffine(image_transformed, H_translate, (x_max-x_min, y_max-y_min))\n",
    "        images_transformed.append(image_transformed)\n",
    "\n",
    "    final_output = np.zeros((y_max-y_min, x_max-x_min, 3), np.uint8)\n",
    "    for i in range(len(images_transformed)):\n",
    "        final_output = final_output + images_transformed[i]\n",
    "\n",
    "    return images_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropImage(img):\n",
    "\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    _,thresh = cv2.threshold(gray,5,255,cv2.THRESH_BINARY)\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for contour in contours:\n",
    "        x,y,w,h = cv2.boundingRect(contours[0])\n",
    "        crop = img[y:y+h,x:x+w]\n",
    "    return crop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinImages(img_array):\n",
    "    image_array = img_array.copy()\n",
    "    N = len(image_array)\n",
    "    image0 = image_array[0]\n",
    "    for i in range(1, N):\n",
    "\n",
    "        print(\"processing image \", i)\n",
    "        image1 = image_array[i] \n",
    "\n",
    "        image_pair = [image0, image1]\n",
    "        \n",
    "        detected_corners, cmaps, corner_images = detectCorners(image_pair, choice)\n",
    "        displayImages(corner_images, \"corners\")\n",
    "        \"\"\"\n",
    "        Perform ANMS: Adaptive Non-Maximal Suppression\n",
    "        Save ANMS output as anms.png\n",
    "        \"\"\"\n",
    "        if (choice == 1):\n",
    "            print(\"Applying ALMS.\")\n",
    "            detected_corners, anms_image = AdaptiveNonMaximalSuppression(image_pair, cmaps, 500)\n",
    "            displayImages(anms_image, \"anms_output\")\n",
    "        else:\n",
    "            print(\"goodFeaturesToTrack is already using ALMS.\") #review\n",
    "\n",
    "        detected_corners0 = detected_corners[0]\n",
    "        detected_corners1 = detected_corners[1]\n",
    "                \n",
    "        matched_pairs = getPairs(image0, image1, detected_corners0, detected_corners1, patch_size = 40, alpha = 0.9 )\n",
    "        showMatches(image0, image1, matched_pairs, partition_width = 20)\n",
    "        \"\"\"\n",
    "        Refine: RANSAC, Estimate Homography\n",
    "        \"\"\"\n",
    "        H,filtered_matched_pairs = filterOutliers(matched_pairs, 20, 0.9, 15)\n",
    "        showMatches(image0, image1, filtered_matched_pairs, partition_width = 20)\n",
    "        \"\"\"\n",
    "        Image Warping + Blending\n",
    "        Save Panorama output as mypano.png\n",
    "        \"\"\"\n",
    "        stitched_image = stitchImagePairs(image0, image1, H)\n",
    "        stitched_image = cropImage(stitched_image)\n",
    "        cv2.imshow(\"pano.jpg\", stitched_image)\n",
    "        cv2.waitKey() \n",
    "        cv2.destroyAllWindows()\n",
    "        cv2.imwrite(\"Results/pano.png\", stitched_image)\n",
    "        image0 = stitched_image\n",
    "\n",
    "    return image0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Add any Command Line arguments here\n",
    "    # Parser = argparse.ArgumentParser()\n",
    "    # Parser.add_argument('--NumFeatures', default=100, help='Number of best features to extract from each image, Default:100')\n",
    "    \n",
    "    # Args = Parser.parse_args()\n",
    "    # NumFeatures = Args.NumFeatures\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    Read a set of images for Panorama stitching\n",
    "    \"\"\"\n",
    "    print(\"main\")\n",
    "    folder_name = \"/home/sakshi/courses/CMSC733/sakshi_p1/Phase1/Data/Train/out\"\n",
    "    use_harris = False\n",
    "    images = readImageSet(folder_name)\n",
    "    displayImages(images, \"image_set\")\n",
    "    N_images = len(images)\n",
    "\n",
    "    choice = 2\n",
    "    if use_harris:\n",
    "        choice = 1\n",
    "\n",
    "    image0 = images[0]\n",
    "\n",
    "    for i in range(1, N_images):\n",
    "\n",
    "        print(\"processing image \", i)\n",
    "        image1 = images[i] \n",
    "\n",
    "        image_pair = [image0, image1]\n",
    "        \n",
    "        detected_corners, cmaps, corner_images = detectCorners(image_pair, choice)\n",
    "        #displayImages(corner_images, \"corners\")\n",
    "        \"\"\"\n",
    "        Perform ANMS: Adaptive Non-Maximal Suppression\n",
    "        Save ANMS output as anms.png\n",
    "        \"\"\"\n",
    "        if (choice == 1):\n",
    "            print(\"Applying ALMS.\")\n",
    "            detected_corners, anms_image = AdaptiveNonMaximalSuppression(image_pair, cmaps, 500)\n",
    "            #displayImages(anms_image, \"anms_output\")\n",
    "        else:\n",
    "            print(\"goodFeaturesToTrack is already using ALMS.\") #review\n",
    "\n",
    "        detected_corners0 = detected_corners[0]\n",
    "        detected_corners1 = detected_corners[1]\n",
    "            \n",
    "        matched_pairs = getPairs(image0, image1, detected_corners0, detected_corners1, patch_size = 40, alpha = 0.9 )\n",
    "        #showMatches(image0, image1, matched_pairs, partition_width = 20)\n",
    "        \"\"\"\n",
    "        Refine: RANSAC, Estimate Homography\n",
    "        \"\"\"\n",
    "        H,filtered_matched_pairs = filterOutliers(matched_pairs, 20, 0.9, 25)\n",
    "        #showMatches(image0, image1, filtered_matched_pairs, partition_width = 20)\n",
    "        \"\"\"\n",
    "        Image Warping + Blending\n",
    "        Save Panorama output as mypano.png\n",
    "        \"\"\"\n",
    "        stitched_image = stitchImagePairs(image0, image1, H)\n",
    "        stitched_image = cropImage(stitched_image)\n",
    "        # cv2.imshow(\"pano.jpg\", stitched_image)\n",
    "        # cv2.waitKey() \n",
    "        # cv2.destroyAllWindows()\n",
    "        cv2.imwrite(\"Results/pano.png\", stitched_image)\n",
    "        image0 = stitched_image\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "main\nReading images from  /home/sakshi/courses/CMSC733/sakshi_p1/Phase1/Data/Train/out\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/sakshi/courses/CMSC733/sakshi_p1/Phase1/Data/Train/out'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-e09c704bfa76>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mfolder_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/sakshi/courses/CMSC733/sakshi_p1/Phase1/Data/Train/out\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0muse_harris\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadImageSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdisplayImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"image_set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mN_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-37b77200f85f>\u001b[0m in \u001b[0;36mreadImageSet\u001b[0;34m(folder_name)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reading images from \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Found \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/sakshi/courses/CMSC733/sakshi_p1/Phase1/Data/Train/out'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "main\nReading images from  /home/sakshi/courses/CMSC733/sakshi_p1/Phase1/Data/Train/Set1\nFound  ['1.jpg', '2.jpg', '3.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(\"main\")\n",
    "folder_name = \"/home/sakshi/courses/CMSC733/sakshi_p1/Phase1/Data/Train/Set1\"\n",
    "use_harris = False\n",
    "images = readImageSet(folder_name)\n",
    "displayImages(images, \"image_set\")\n",
    "N_images = len(images)\n",
    "choice = 2\n",
    "if use_harris:\n",
    "    choice = 1\n",
    "\n",
    "#image0 = images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2  and  1\n"
     ]
    }
   ],
   "source": [
    "N_first_half = round(N_images/2)\n",
    "N_second_half = N_images - N_first_half\n",
    "print(N_first_half, \" and \", N_second_half)\n",
    "\n",
    "N_first_half_images = []\n",
    "N_second_half_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "N =  3  N_half =  2\n",
      "combining:  0 1\n",
      "processing image  1\n",
      "detecting corners ...\n",
      "using Shi-Tomashi corner detection method.\n",
      "using Shi-Tomashi corner detection method.\n",
      "goodFeaturesToTrack is already using ALMS.\n",
      "width =  450 , height =  600\n",
      "width =  450 , height =  600\n",
      "all corner 1 len =  616\n",
      "all corner 2 len =  711\n",
      "matched pairs num =  517\n",
      "iterations =  4000\n",
      "Number of pairs after filtering =  229\n",
      "inbuilt =  [[-6.51661112e-01 -2.28634018e+00  6.49829969e+02]\n",
      " [-1.86012425e-01 -6.53463614e-01  1.85314843e+02]\n",
      " [-9.86728387e-04 -3.54897705e-03  1.00000000e+00]]\n",
      "Computed =  [[ 1.08974381e+00  8.08603106e-02 -3.67773262e+01]\n",
      " [-7.92567896e-03  1.10399181e+00 -2.50788461e+02]\n",
      " [-4.07087833e-05  2.98362149e-04  1.00000000e+00]]\n",
      "shapes\n",
      "(450, 600, 3)\n",
      "(450, 600, 3)\n",
      "transformed points =  [[[-3.6777325e+01 -2.5078847e+02]]\n",
      "\n",
      " [[-3.4399995e-01  2.1688785e+02]]\n",
      "\n",
      " [[ 5.8878528e+02  2.1737633e+02]]\n",
      "\n",
      " [[ 6.3251837e+02 -2.6194186e+02]]]\n",
      "min, max\n",
      "-36 -261\n",
      "632 450\n",
      "combining:  2 1\n",
      "processing image  1\n",
      "detecting corners ...\n",
      "using Shi-Tomashi corner detection method.\n",
      "using Shi-Tomashi corner detection method.\n",
      "goodFeaturesToTrack is already using ALMS.\n",
      "width =  450 , height =  600\n",
      "width =  450 , height =  600\n",
      "all corner 1 len =  852\n",
      "all corner 2 len =  711\n",
      "matched pairs num =  714\n",
      "iterations =  4000\n",
      "Number of pairs after filtering =  237\n",
      "inbuilt =  [[-5.88228391e-01 -1.18586832e+00  2.62955785e+02]\n",
      " [-3.63974056e-01 -7.40132008e-01  1.63696289e+02]\n",
      " [-2.23239227e-03 -4.51913245e-03  1.00000000e+00]]\n",
      "Computed =  [[ 1.34489680e+00  2.87130867e-02 -3.25355775e+02]\n",
      " [ 1.18868172e-01  1.33880289e+00 -6.59324810e+01]\n",
      " [ 5.53055171e-04  2.58533403e-04  1.00000000e+00]]\n",
      "shapes\n",
      "(450, 600, 3)\n",
      "(450, 600, 3)\n",
      "transformed points =  [[[-325.35577   -65.93248 ]]\n",
      "\n",
      " [[-279.8743    480.61417 ]]\n",
      "\n",
      " [[ 341.4669    419.73553 ]]\n",
      "\n",
      " [[ 361.59357     4.045869]]]\n",
      "min, max\n",
      "-325 -65\n",
      "600 480\n",
      "processing image  1\n",
      "detecting corners ...\n",
      "using Shi-Tomashi corner detection method.\n",
      "using Shi-Tomashi corner detection method.\n",
      "goodFeaturesToTrack is already using ALMS.\n",
      "width =  711 , height =  668\n",
      "width =  545 , height =  925\n",
      "all corner 1 len =  838\n",
      "all corner 2 len =  1000\n",
      "matched pairs num =  812\n",
      "iterations =  4000\n",
      "Number of pairs after filtering =  548\n",
      "inbuilt =  [[-9.34793473e-01 -3.23934046e+00  8.09488655e+02]\n",
      " [-2.27816660e-01 -1.11015514e+00  2.21061301e+02]\n",
      " [-1.02824003e-03 -5.03124204e-03  1.00000000e+00]]\n",
      "Computed =  [[ 1.00000000e+00  1.52941705e-15  2.89000000e+02]\n",
      " [-5.73665421e-16  1.00000000e+00 -1.96000000e+02]\n",
      " [-6.43094114e-19  4.11568851e-18  1.00000000e+00]]\n",
      "shapes\n",
      "(711, 668, 3)\n",
      "(545, 925, 3)\n",
      "transformed points =  [[[ 289. -196.]]\n",
      "\n",
      " [[ 289.  515.]]\n",
      "\n",
      " [[ 957.  515.]]\n",
      "\n",
      " [[ 957. -196.]]]\n",
      "min, max\n",
      "0 -196\n",
      "957 545\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while N_images is not 2:\n",
    "    print(\"N = \", N_images, \" N_half = \", N_first_half)\n",
    "    merged_images = []\n",
    "    for n in range(0, N_first_half, 2):\n",
    "        if (n+1) <= N_first_half:\n",
    "            img_array = images[n:n+2]\n",
    "            print(\"combining: \", n, n+1)\n",
    "            I = joinImages(img_array)\n",
    "            merged_images.append(I)\n",
    "        else:\n",
    "            print(\"adding: \", n)\n",
    "            merged_images.append(images[n])\n",
    "\n",
    "    for n in range(N_second_half, N_images, 2):\n",
    "        if (n+1) <= N_images:\n",
    "            img_array = images[n:n+2]\n",
    "            img_array.reverse()\n",
    "            print(\"combining: \", n+1, n)\n",
    "            I = joinImages(img_array)\n",
    "            merged_images.append(I)\n",
    "        else:\n",
    "            print(\"adding: \", n, n)\n",
    "            merged_images.append(images[n])\n",
    " \n",
    "    images = merged_images\n",
    "    N_images = len(images)\n",
    "    N_first_half = round(N_images/2)\n",
    "    N_second_half = N_images - N_first_half\n",
    "\n",
    "\n",
    "final = joinImages(merged_images)\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(0, N_first_half, 2):\n",
    "    if (n+1) <= N_first_half:\n",
    "        img_array = images[n:n+2]\n",
    "        I = joinImages(img_array)\n",
    "        N_first_half_images.append(I)\n",
    "    else:\n",
    "        N_first_half_images.append(images[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing image  1\n",
      "detecting corners ...\n",
      "using Shi-Tomashi corner detection method.\n",
      "using Shi-Tomashi corner detection method.\n",
      "goodFeaturesToTrack is already using ALMS.\n",
      "width =  807 , height =  605\n",
      "width =  807 , height =  605\n",
      "all corner 1 len =  360\n",
      "all corner 2 len =  435\n",
      "matched pairs num =  335\n",
      "iterations =  4000\n",
      "Number of pairs after filtering =  132\n",
      "inbuilt =  [[-1.06160707e+03  8.11626687e+02  6.08756345e+03]\n",
      " [-1.60875462e+03  1.23145835e+03  8.14117585e+03]\n",
      " [-3.52523345e+00  2.71748615e+00  1.00000000e+00]]\n",
      "Computed =  [[ 7.34088829e-01 -1.19466047e-02  2.10696588e+02]\n",
      " [-1.46420120e-01  9.07684534e-01  3.81698310e+01]\n",
      " [-3.89212476e-04 -7.70660364e-06  1.00000000e+00]]\n",
      "shapes\n",
      "(807, 605, 3)\n",
      "(807, 605, 3)\n",
      "transformed points =  [[[210.6966   38.16983]]\n",
      "\n",
      " [[202.31392 775.4942 ]]\n",
      "\n",
      " [[850.81537 899.48645]]\n",
      "\n",
      " [[856.50446 -65.94192]]]\n",
      "min, max\n",
      "0 -65\n",
      "856 899\n",
      "processing image  1\n",
      "detecting corners ...\n",
      "using Shi-Tomashi corner detection method.\n",
      "using Shi-Tomashi corner detection method.\n",
      "goodFeaturesToTrack is already using ALMS.\n",
      "width =  807 , height =  605\n",
      "width =  807 , height =  605\n",
      "all corner 1 len =  338\n",
      "all corner 2 len =  371\n",
      "matched pairs num =  303\n",
      "iterations =  4000\n",
      "Number of pairs after filtering =  213\n",
      "inbuilt =  [[-5.64704695e-01 -1.41582635e-01  3.61545004e+02]\n",
      " [-9.46315307e-01 -1.57742839e-01  5.46116206e+02]\n",
      " [-1.62438234e-03 -3.54812627e-04  1.00000000e+00]]\n",
      "Computed =  [[ 8.77200846e-01  1.25460193e-02  1.36910614e+02]\n",
      " [-8.63879214e-02  9.43196510e-01  2.23412495e+01]\n",
      " [-2.33955060e-04 -2.83885137e-06  1.00000000e+00]]\n",
      "shapes\n",
      "(807, 605, 3)\n",
      "(807, 605, 3)\n",
      "transformed points =  [[[136.91061   22.34125 ]]\n",
      "\n",
      " [[147.37288  785.2999  ]]\n",
      "\n",
      " [[791.60065  854.082   ]]\n",
      "\n",
      " [[777.69415  -34.857235]]]\n",
      "min, max\n",
      "0 -34\n",
      "791 854\n"
     ]
    }
   ],
   "source": [
    "for n in range(N_second_half, N_images, 2):\n",
    "    if (n+1) <= N_images:\n",
    "        img_array = images[n:n+2]\n",
    "        img_array.reverse()\n",
    "        I = joinImages(img_array)\n",
    "        N_second_half_images.append(I)\n",
    "    else:\n",
    "        N_second_half_images.append(images[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "len(N_first_half_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing image  1\n",
      "detecting corners ...\n",
      "using Shi-Tomashi corner detection method.\n",
      "using Shi-Tomashi corner detection method.\n",
      "goodFeaturesToTrack is already using ALMS.\n",
      "width =  807 , height =  605\n",
      "width =  807 , height =  605\n",
      "all corner 1 len =  489\n",
      "all corner 2 len =  561\n",
      "matched pairs num =  442\n",
      "iterations =  4000\n",
      "Number of pairs after filtering =  117\n",
      "inbuilt =  [[ 8.98932359e-01 -8.09269316e-01  2.35080040e+02]\n",
      " [ 1.94672202e+00 -1.72142241e+00  4.96172856e+02]\n",
      " [ 3.89482378e-03 -3.46407303e-03  1.00000000e+00]]\n",
      "Computed =  [[ 1.24737379e+00 -8.84943188e-02 -3.29908594e+02]\n",
      " [ 1.02994597e-01  1.15563818e+00 -4.28231262e+01]\n",
      " [ 4.47103777e-04 -7.36595427e-05  1.00000000e+00]]\n",
      "shapes\n",
      "(807, 605, 3)\n",
      "(807, 605, 3)\n",
      "transformed points =  [[[-329.9086    -42.823128]]\n",
      "\n",
      " [[-426.6872    946.01086 ]]\n",
      "\n",
      " [[ 291.76028   786.1649  ]]\n",
      "\n",
      " [[ 334.3198     15.339346]]]\n",
      "min, max\n",
      "-426 -42\n",
      "605 946\n"
     ]
    }
   ],
   "source": [
    "img_array1 = images[0:2]\n",
    "I1 = joinImages(img_array1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing image  1\n",
      "detecting corners ...\n",
      "using Shi-Tomashi corner detection method.\n",
      "using Shi-Tomashi corner detection method.\n",
      "goodFeaturesToTrack is already using ALMS.\n",
      "width =  807 , height =  605\n",
      "width =  807 , height =  605\n",
      "all corner 1 len =  429\n",
      "all corner 2 len =  422\n",
      "matched pairs num =  386\n",
      "iterations =  4000\n",
      "Number of pairs after filtering =  186\n",
      "inbuilt =  [[-7.28248872e+00 -3.45472708e-01  8.90699601e+02]\n",
      " [-1.76804179e+01 -1.22585004e+00  2.44685023e+03]\n",
      " [-9.89539929e-03 -1.22353055e-04  1.00000000e+00]]\n",
      "Computed =  [[ 1.28889178e+00  2.46378505e-02 -2.56829274e+02]\n",
      " [ 1.53408301e-01  1.17176000e+00 -5.61976774e+01]\n",
      " [ 5.42031499e-04 -8.65526178e-05  1.00000000e+00]]\n",
      "shapes\n",
      "(807, 605, 3)\n",
      "(807, 605, 3)\n",
      "transformed points =  [[[-256.82928   -56.197678]]\n",
      "\n",
      " [[-254.73956   956.20135 ]]\n",
      "\n",
      " [[ 431.47696   780.7324  ]]\n",
      "\n",
      " [[ 393.80887    27.572515]]]\n",
      "min, max\n",
      "-256 -56\n",
      "605 956\n"
     ]
    }
   ],
   "source": [
    "img_array2 = images[2:4]\n",
    "I2 = joinImages(img_array2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing image  1\n",
      "detecting corners ...\n",
      "using Shi-Tomashi corner detection method.\n",
      "using Shi-Tomashi corner detection method.\n",
      "goodFeaturesToTrack is already using ALMS.\n",
      "width =  807 , height =  605\n",
      "width =  807 , height =  605\n",
      "all corner 1 len =  360\n",
      "all corner 2 len =  435\n",
      "matched pairs num =  335\n",
      "iterations =  4000\n",
      "Number of pairs after filtering =  130\n",
      "inbuilt =  [[-1.06160707e+03  8.11626687e+02  6.08756345e+03]\n",
      " [-1.60875462e+03  1.23145835e+03  8.14117585e+03]\n",
      " [-3.52523345e+00  2.71748615e+00  1.00000000e+00]]\n",
      "Computed =  [[ 7.31079802e-01 -4.76643136e-03  1.95554946e+02]\n",
      " [-1.53461429e-01  8.42666605e-01  5.40036140e+01]\n",
      " [-4.01958992e-04 -6.82421939e-05  1.00000000e+00]]\n",
      "shapes\n",
      "(807, 605, 3)\n",
      "(807, 605, 3)\n",
      "transformed points =  [[[195.55495   54.003613]]\n",
      "\n",
      " [[202.88141  776.8159  ]]\n",
      "\n",
      " [[903.4809   913.7121  ]]\n",
      "\n",
      " [[842.81946  -51.321075]]]\n",
      "min, max\n",
      "0 -51\n",
      "903 913\n"
     ]
    }
   ],
   "source": [
    "img_array3 = images[4:6]\n",
    "img_array3.reverse()\n",
    "I3 = joinImages(img_array3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing image  1\n",
      "detecting corners ...\n",
      "using Shi-Tomashi corner detection method.\n",
      "using Shi-Tomashi corner detection method.\n",
      "goodFeaturesToTrack is already using ALMS.\n",
      "width =  807 , height =  605\n",
      "width =  807 , height =  605\n",
      "all corner 1 len =  338\n",
      "all corner 2 len =  371\n",
      "matched pairs num =  303\n",
      "iterations =  4000\n",
      "Number of pairs after filtering =  213\n",
      "inbuilt =  [[-5.64704695e-01 -1.41582635e-01  3.61545004e+02]\n",
      " [-9.46315307e-01 -1.57742839e-01  5.46116206e+02]\n",
      " [-1.62438234e-03 -3.54812627e-04  1.00000000e+00]]\n",
      "Computed =  [[ 9.09417792e-01  3.37706749e-02  1.28406299e+02]\n",
      " [-6.45282732e-02  9.83695166e-01  1.05864661e+01]\n",
      " [-2.31417699e-04  4.67652520e-05  1.00000000e+00]]\n",
      "shapes\n",
      "(807, 605, 3)\n",
      "(807, 605, 3)\n",
      "transformed points =  [[[128.4063    10.586466]]\n",
      "\n",
      " [[149.99837  775.17377 ]]\n",
      "\n",
      " [[786.2671   852.5807  ]]\n",
      "\n",
      " [[789.08154  -33.085342]]]\n",
      "min, max\n",
      "0 -33\n",
      "789 852\n"
     ]
    }
   ],
   "source": [
    "img_array4 = images[6:8]\n",
    "img_array4.reverse()\n",
    "I4 = joinImages(img_array4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing image  1\n",
      "detecting corners ...\n",
      "using Shi-Tomashi corner detection method.\n",
      "using Shi-Tomashi corner detection method.\n",
      "goodFeaturesToTrack is already using ALMS.\n",
      "width =  988 , height =  1031\n",
      "width =  1012 , height =  861\n",
      "all corner 1 len =  790\n",
      "all corner 2 len =  595\n",
      "matched pairs num =  773\n",
      "iterations =  4000\n",
      "Number of pairs after filtering =  22\n",
      "inbuilt =  [[ 5.88467183e-02 -7.45742259e-01  4.35425770e+02]\n",
      " [ 7.29205167e-02 -9.25691400e-01  5.40319259e+02]\n",
      " [ 1.35208058e-04 -1.71334941e-03  1.00000000e+00]]\n",
      "Computed =  [[ 1.33506902e+00 -2.50732421e-01 -7.90384054e+02]\n",
      " [ 3.62198976e-01  5.13479223e-01 -6.00996968e+01]\n",
      " [ 6.13564893e-04 -1.13412748e-03  1.00000000e+00]]\n",
      "shapes\n",
      "(988, 1031, 3)\n",
      "(1012, 861, 3)\n",
      "transformed points =  [[[ -790.38403    -60.099697]]\n",
      "\n",
      " [[ 8613.718    -3710.7979  ]]\n",
      "\n",
      " [[  660.7498    1602.611   ]]\n",
      "\n",
      " [[  358.98404    191.92102 ]]]\n",
      "min, max\n",
      "-790 -3710\n",
      "8613 1602\n"
     ]
    }
   ],
   "source": [
    "IC12 = [I1, I2]\n",
    "IC1 = joinImages(IC12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing image  1\n",
      "detecting corners ...\n",
      "using Shi-Tomashi corner detection method.\n",
      "using Shi-Tomashi corner detection method.\n",
      "goodFeaturesToTrack is already using ALMS.\n",
      "width =  885 , height =  789\n",
      "width =  964 , height =  903\n",
      "all corner 1 len =  426\n",
      "all corner 2 len =  592\n",
      "matched pairs num =  392\n",
      "iterations =  4000\n",
      "Number of pairs after filtering =  112\n",
      "inbuilt =  [[ 1.42109674e-01 -7.21047181e-01  4.57689441e+02]\n",
      " [ 4.80491679e-02 -2.57451644e-01  1.64379120e+02]\n",
      " [ 3.07844309e-04 -1.57427599e-03  1.00000000e+00]]\n",
      "Computed =  [[ 6.66450246e-01 -7.69769816e-02  3.19239967e+02]\n",
      " [-2.46396681e-01  7.83620457e-01  7.91178940e+01]\n",
      " [-5.69740419e-04 -1.55506792e-04  1.00000000e+00]]\n",
      "shapes\n",
      "(885, 789, 3)\n",
      "(964, 903, 3)\n",
      "transformed points =  [[[ 319.23996   79.1179 ]]\n",
      "\n",
      " [[ 291.1899   895.92194]]\n",
      "\n",
      " [[1881.8993  1400.5406 ]]\n",
      "\n",
      " [[1535.1642  -209.43571]]]\n",
      "min, max\n",
      "0 -209\n",
      "1881 1400\n"
     ]
    }
   ],
   "source": [
    "IC43 = [I4, I3]\n",
    "IC2 = joinImages(IC43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing image  1\n",
      "detecting corners ...\n",
      "using Shi-Tomashi corner detection method.\n",
      "using Shi-Tomashi corner detection method.\n",
      "goodFeaturesToTrack is already using ALMS.\n",
      "width =  1626 , height =  1651\n",
      "width =  1605 , height =  1872\n",
      "all corner 1 len =  1000\n",
      "all corner 2 len =  788\n",
      "matched pairs num =  980\n",
      "iterations =  4000\n",
      "Number of pairs after filtering =  188\n",
      "inbuilt =  [[ 1.93877155e+01  3.46626143e+00 -9.11852237e+03]\n",
      " [-1.57628910e+01 -2.65508304e+00  7.26803996e+03]\n",
      " [-2.43668379e-03 -2.71073388e-04  1.00000000e+00]]\n",
      "Computed =  [[ 2.88200909e+00  3.95743743e-02 -3.61977303e+03]\n",
      " [ 7.62914634e-01  2.65818080e+00 -5.87322554e+02]\n",
      " [ 1.14008215e-03  5.85405181e-05  1.00000000e+00]]\n",
      "shapes\n",
      "(1626, 1651, 3)\n",
      "(1605, 1872, 3)\n",
      "transformed points =  [[[-3619.773    -587.3226 ]]\n",
      "\n",
      " [[-3246.4094   3410.2668 ]]\n",
      "\n",
      " [[  403.9587   1677.4188 ]]\n",
      "\n",
      " [[  394.974     233.23567]]]\n",
      "min, max\n",
      "-3619 -587\n",
      "1872 3410\n"
     ]
    }
   ],
   "source": [
    "ICC = [IC1, IC2]\n",
    "IC = joinImages(ICC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing image  1\n",
      "detecting corners ...\n",
      "using Shi-Tomashi corner detection method.\n",
      "using Shi-Tomashi corner detection method.\n",
      "goodFeaturesToTrack is already using ALMS.\n",
      "width =  1948 , height =  2136\n",
      "width =  1375 , height =  1345\n",
      "all corner 1 len =  1000\n",
      "all corner 2 len =  731\n",
      "matched pairs num =  984\n",
      "iterations =  4000\n",
      "Number of pairs after filtering =  170\n",
      "inbuilt =  [[-4.90308820e-01 -2.99605418e-01  8.66904404e+02]\n",
      " [-1.22731513e+00 -7.43721233e-01  2.16654045e+03]\n",
      " [-5.65312082e-04 -3.48434455e-04  1.00000000e+00]]\n",
      "Computed =  [[ 8.93950561e+00  4.38338678e-01 -1.56259688e+04]\n",
      " [ 2.03247349e+00  8.96887163e+00 -4.08980610e+03]\n",
      " [ 3.61705814e-03  7.39540669e-04  1.00000000e+00]]\n",
      "shapes\n",
      "(1948, 2136, 3)\n",
      "(1375, 1345, 3)\n",
      "transformed points =  [[[-15625.969     -4089.8062  ]]\n",
      "\n",
      " [[ -6052.5825     5482.8394  ]]\n",
      "\n",
      " [[   425.18372    1743.2389  ]]\n",
      "\n",
      " [[   397.52472      28.828356]]]\n",
      "min, max\n",
      "-15625 -4089\n",
      "1345 5482\n"
     ]
    }
   ],
   "source": [
    "IAC = [I1, I2]\n",
    "IC = joinImages(IAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'IC' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c737c70bba87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_array_comb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mIC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mICf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoinImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array_comb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'IC' is not defined"
     ]
    }
   ],
   "source": [
    "img_array_comb1 = [IC, I3]\n",
    "ICf = joinImages(img_array_comb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing image  1\n",
      "detecting corners ...\n",
      "using Shi-Tomashi corner detection method.\n",
      "using Shi-Tomashi corner detection method.\n",
      "goodFeaturesToTrack is already using ALMS.\n",
      "width =  807 , height =  605\n",
      "width =  807 , height =  605\n",
      "all corner 1 len =  429\n",
      "all corner 2 len =  561\n",
      "matched pairs num =  386\n",
      "iterations =  4000\n",
      "Number of pairs after filtering =  49\n",
      "inbuilt =  [[-4.12467020e-01 -3.25468518e-01  3.34338168e+02]\n",
      " [-5.74378184e-01 -4.53123039e-01  4.65206922e+02]\n",
      " [-1.23549400e-03 -9.72509615e-04  1.00000000e+00]]\n",
      "Computed =  [[ 7.51125786e-01  8.83055831e-02  2.35769935e+02]\n",
      " [-1.43518048e-01  1.00013120e+00 -1.11578408e+01]\n",
      " [-4.83287111e-04  8.51947064e-05  1.00000000e+00]]\n",
      "shapes\n",
      "(807, 605, 3)\n",
      "(807, 605, 3)\n",
      "transformed points =  [[[ 235.76993   -11.157841]]\n",
      "\n",
      " [[ 287.28134   744.74524 ]]\n",
      "\n",
      " [[ 980.8082    913.38617 ]]\n",
      "\n",
      " [[ 975.39575  -138.4747  ]]]\n",
      "min, max\n",
      "0 -138\n",
      "980 913\n",
      "processing image  2\n",
      "detecting corners ...\n",
      "using Shi-Tomashi corner detection method.\n",
      "using Shi-Tomashi corner detection method.\n",
      "goodFeaturesToTrack is already using ALMS.\n",
      "width =  1051 , height =  980\n",
      "width =  807 , height =  605\n",
      "all corner 1 len =  855\n",
      "all corner 2 len =  429\n",
      "matched pairs num =  821\n",
      "iterations =  4000\n",
      "Number of pairs after filtering =  163\n",
      "inbuilt =  [[-1.34497575e+01 -5.41484456e+00  6.09160828e+03]\n",
      " [-1.05205573e+01 -9.24193481e+00  8.73012950e+03]\n",
      " [-1.43362560e-02  1.04848624e-03  1.00000000e+00]]\n",
      "Computed =  [[ 1.28465744e+00 -8.45032437e-02 -2.95317757e+02]\n",
      " [ 1.89161406e-01  1.10810078e+00 -1.84733320e+02]\n",
      " [ 5.97286447e-04 -1.36480257e-04  1.00000000e+00]]\n",
      "shapes\n",
      "(1051, 980, 3)\n",
      "(807, 605, 3)\n",
      "transformed points =  [[[-2.9531775e+02 -1.8473332e+02]]\n",
      "\n",
      " [[-4.4845779e+02  1.1439729e+03]]\n",
      "\n",
      " [[ 6.0672284e+02  8.0814124e+02]]\n",
      "\n",
      " [[ 6.0784821e+02  4.0676296e-01]]]\n",
      "min, max\n",
      "-448 -184\n",
      "607 1143\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 3):\n",
    "\n",
    "    print(\"processing image \", i)\n",
    "    image1 = images[i] \n",
    "\n",
    "    image_pair = [image0, image1]\n",
    "      \n",
    "    detected_corners, cmaps, corner_images = detectCorners(image_pair, choice)\n",
    "    displayImages(corner_images, \"corners\")\n",
    "    \"\"\"\n",
    "    Perform ANMS: Adaptive Non-Maximal Suppression\n",
    "    Save ANMS output as anms.png\n",
    "    \"\"\"\n",
    "    if (choice == 1):\n",
    "        print(\"Applying ALMS.\")\n",
    "        detected_corners, anms_image = AdaptiveNonMaximalSuppression(image_pair, cmaps, 500)\n",
    "        displayImages(anms_image, \"anms_output\")\n",
    "    else:\n",
    "        print(\"goodFeaturesToTrack is already using ALMS.\") #review\n",
    "\n",
    "    detected_corners0 = detected_corners[0]\n",
    "    detected_corners1 = detected_corners[1]\n",
    "            \n",
    "    matched_pairs = getPairs(image0, image1, detected_corners0, detected_corners1, patch_size = 40, alpha = 0.9 )\n",
    "    showMatches(image0, image1, matched_pairs, partition_width = 20)\n",
    "    \"\"\"\n",
    "    Refine: RANSAC, Estimate Homography\n",
    "    \"\"\"\n",
    "    H,filtered_matched_pairs = filterOutliers(matched_pairs, 20, 0.9, 5)\n",
    "    showMatches(image0, image1, filtered_matched_pairs, partition_width = 20)\n",
    "    \"\"\"\n",
    "    Image Warping + Blending\n",
    "    Save Panorama output as mypano.png\n",
    "    \"\"\"\n",
    "    stitched_image = stitchImagePairs(image0, image1, H)\n",
    "    stitched_image = cropImage(stitched_image)\n",
    "    cv2.imshow(\"pano.jpg\", stitched_image)\n",
    "    cv2.waitKey() \n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.imwrite(\"Results/pano.png\", stitched_image)\n",
    "    image0 = stitched_image\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"pano.jpg\", IC1)\n",
    "cv2.waitKey() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1187, 1149, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "image0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(807, 605, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "images[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 0 1149 1187\n"
     ]
    }
   ],
   "source": [
    "gray = cv2.cvtColor(image0,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "_,thresh = cv2.threshold(gray,5,255,cv2.THRESH_BINARY)\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "cv2.imshow(\"pano.jpg\", thresh)\n",
    "cv2.waitKey() \n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
    "# Find bounding box and extract ROI\n",
    "for c in cnts:\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    print(x,y,w,h)\n",
    "    ROI = image0[y:y+h, x:x+w]\n",
    "    break\n",
    "\n",
    "cv2.imshow('ROI',ROI)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image0\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "_, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU + cv2.THRESH_BINARY)\n",
    "cv2.imshow('ROI',thresh)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Find contour and sort by contour area\n",
    "cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "# Find bounding box and extract ROI\n",
    "for c in cnts:\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    ROI = image[x:x+w, y:y+h]\n",
    "    break\n",
    "\n",
    "cv2.imshow('ROI',ROI)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([[[101,   0]],\n",
       " \n",
       "        [[101,  11]],\n",
       " \n",
       "        [[100,  12]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[123,   3]],\n",
       " \n",
       "        [[122,   2]],\n",
       " \n",
       "        [[122,   0]]], dtype=int32)]"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cnts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[101,   0],\n",
       "       [101,  11],\n",
       "       [100,  12],\n",
       "       ...,\n",
       "       [123,   3],\n",
       "       [122,   2],\n",
       "       [122,   0]], dtype=int32)"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "c.reshape(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "detecting corners ...\nusing Shi-Tomashi corner detection method.\nusing Shi-Tomashi corner detection method.\n"
     ]
    }
   ],
   "source": [
    "    image1 = images[2] \n",
    "\n",
    "    image_pair = [image1, image0]\n",
    "      \n",
    "    detected_corners, cmaps, corner_images = detectCorners(image_pair, choice)\n",
    "    displayImages(corner_images, \"corners\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "width =  807 , height =  605\n",
      "width =  1196 , height =  1039\n",
      "all corner 1 len =  429\n",
      "all corner 2 len =  787\n",
      "matched pairs num =  386\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    detected_corners1 = detected_corners[0]\n",
    "    detected_corners0 = detected_corners[1]\n",
    "            \n",
    "    matched_pairs = getPairs(image1, image0, detected_corners1, detected_corners0, patch_size = 40, alpha = 0.9 )\n",
    "    showMatches(image1, image0, matched_pairs, partition_width = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "iterations =  4000\n",
      "Number of pairs after filtering =  48\n",
      "inbuilt =  [[-1.34594960e+00 -6.71089791e-01  8.97263198e+02]\n",
      " [-2.56094495e-01 -9.16569977e-02  1.54123872e+02]\n",
      " [-1.50887966e-03 -7.39465712e-04  1.00000000e+00]]\n",
      "Computed =  [[ 2.73530660e-01 -2.81288938e-02  6.79954177e+02]\n",
      " [-3.25652761e-01  7.62050446e-01  2.09391426e+02]\n",
      " [-6.70557712e-04 -1.09794557e-04  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "    H,filtered_matched_pairs = filterOutliers(matched_pairs, 20, 0.9, 5)\n",
    "    showMatches(image1, image0, filtered_matched_pairs, partition_width = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shapes\n(807, 605, 3)\n(1196, 1039, 3)\ntransformed points =  [[[ 679.95416   209.39143 ]]\n\n [[ 721.1512    904.50946 ]]\n\n [[1626.9064   1240.5297  ]]\n\n [[1422.5514     20.816496]]]\nmin, max\n0 0\n1626 1240\n"
     ]
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    Image Warping + Blending\n",
    "    Save Panorama output as mypano.png\n",
    "    \"\"\"\n",
    "    stitched_image = stitchImagePairs(image1, image0, H)\n",
    "    stitched_image = cropImage(stitched_image)\n",
    "    cv2.imshow(\"pano.jpg\", stitched_image)\n",
    "    cv2.waitKey() \n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.imwrite(\"Results/pano.png\", stitched_image)\n",
    "    image0 = stitched_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}